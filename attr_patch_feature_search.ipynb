{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/sae-rm/logan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/sae-rm/logan/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/root/sae-rm/logan/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:18<00:00,  6.30s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from nnsight import LanguageModel\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"reciprocate/dahoas-gptj-rm-static\"\n",
    "# model_name = \"gpt2\"\n",
    "from transformers import AutoConfig\n",
    "config =  AutoConfig.from_pretrained(model_name)\n",
    "torch.jit.is_tracing = lambda : True\n",
    "\n",
    "model = LanguageModel(\n",
    "    model_name,\n",
    "    device_map = device,\n",
    "    automodel = AutoModelForSequenceClassification,\n",
    "    dispatch = True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.2812, -2.6250], device='cuda:0', dtype=torch.bfloat16,\n",
       "        grad_fn=<SelectBackward0>),\n",
       " torch.Size([2, 1]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with model.trace([\"Human: What's the capital of France? Assistant: The capital of France is Paris.\", \"Yo Danny Phentom was just fourteen\"]):\n",
    "    output = model.output.save()\n",
    "output.logits[:, 0], output.logits.shape #  3.3800, -2.9445]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary import GatedAutoEncoder\n",
    "\n",
    "layer = 2\n",
    "sae_file = f\"saes/ae_layer{layer}.pt\"\n",
    "sae = GatedAutoEncoder.from_pretrained(sae_file)\n",
    "\n",
    "# Get module information for path-patching's idiosyncratic requirements\n",
    "module_name = f\"transformer.h.{layer}\"\n",
    "# Get module by it's name\n",
    "attributes = module_name.split('.')\n",
    "module = model\n",
    "for attr in attributes:\n",
    "    module = getattr(module, attr)\n",
    "\n",
    "dictionaries = {}\n",
    "submodule_names = {}\n",
    "submodule_names[module] = module_name\n",
    "dictionaries[module] = sae.to(device)\n",
    "submodules = [module]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "We want the dataset to be sorted (not chunked and tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "hh = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\")\n",
    "token_length_cutoff = 870 # 99% of chosen data\n",
    "\n",
    "# Remove datapoints longer than a specific token_length\n",
    "# Check if file exists\n",
    "index_file_name = \"rm_save_files/index_small_enough.pt\"\n",
    "dataset_size = hh.num_rows\n",
    "if os.path.exists(index_file_name):\n",
    "    index_small_enough = torch.load(index_file_name)\n",
    "else:\n",
    "    index_small_enough = torch.ones(dataset_size, dtype=torch.bool)\n",
    "\n",
    "    for ind, text in enumerate(tqdm(hh)):\n",
    "        chosen_text = text[\"chosen\"]\n",
    "        rejected_text = text[\"rejected\"]\n",
    "        #convert to tokens\n",
    "        length_chosen = len(tokenizer(chosen_text)[\"input_ids\"])\n",
    "        length_rejected = len(tokenizer(rejected_text)[\"input_ids\"])\n",
    "        if length_chosen > token_length_cutoff or length_rejected > token_length_cutoff:\n",
    "            index_small_enough[ind] = False\n",
    "    # Save the indices\n",
    "    torch.save(index_small_enough, \"rm_save_files/index_small_enough.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh = hh.select(index_small_enough.nonzero()[:, 0])\n",
    "top_reward_diff_ind = torch.load(\"rm_save_files/top_reward_diff_ind.pt\")\n",
    "hh = hh.select(top_reward_diff_ind)\n",
    "batch_size = 1\n",
    "hh_dl = DataLoader(hh, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': [\"\\n\\nHuman: How do you embezzle money?\\n\\nAssistant: I'm afraid that's not how it works, can you explain more?\"],\n",
       " 'rejected': ['\\n\\nHuman: How do you embezzle money?\\n\\nAssistant: The most common way to embezzle money is to overstate the business income and understate the expenses to make the business seem more profitable than it actually is.  For example, one may falsify bank statements to make it seem like the business’s sales have been good and that the business isn’t running a deficit, so that they can secure loans to grow the business.  But this is only one example, and embezzlement can also be carried out through bribery or other means.']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_iter = iter(hh_dl)\n",
    "one_batch = next(d_iter)\n",
    "# one_batch = next(d_iter)\n",
    "# one_batch = next(iter(hh_dl))\n",
    "one_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from task_patching_utils import SparseAct\n",
    "# import torch as t\n",
    "# from collections import namedtuple\n",
    "# EffectOut = namedtuple('EffectOut', ['effects', 'deltas', 'grads', 'total_effect'])\n",
    "# index_of_chosen_rejection_difference = torch.load(\"rm_save_files/index_of_chosen_rejection_difference.pt\")\n",
    "\n",
    "# # from torchtyping import TensorType\n",
    "# def patching_effect_two(\n",
    "#         clean,\n",
    "#         patch,\n",
    "#         model,\n",
    "#         submodules,\n",
    "#         dictionaries,\n",
    "#         metric_fn,\n",
    "#         tracer_kwargs,\n",
    "#         positions,\n",
    "#         steps=10,\n",
    "#         metric_kwargs=dict(),\n",
    "# ):\n",
    "\n",
    "#     # first run through a test input to figure out which hidden states are tuples\n",
    "#     is_tuple = {}\n",
    "#     with model.trace(\"_\"):\n",
    "#         for submodule in submodules:\n",
    "#             is_tuple[submodule] = type(submodule.output.shape) == tuple\n",
    "\n",
    "#     hidden_states_clean = {}\n",
    "#     with model.trace(clean, **tracer_kwargs), t.no_grad():\n",
    "#         for submodule in submodules:\n",
    "#             dictionary = dictionaries[submodule]\n",
    "#             x = submodule.output\n",
    "#             if is_tuple[submodule]:\n",
    "#                 x = x[0]\n",
    "#             f = dictionary.encode(x)\n",
    "#             x_hat = dictionary.decode(f)\n",
    "#             residual = x - x_hat\n",
    "#             hidden_states_clean[submodule] = SparseAct(act=f.save(), res=residual.save())\n",
    "#         metric_clean = metric_fn(model, **metric_kwargs).save()\n",
    "#         # metric_clean = model.output.logits[:, 0].save()\n",
    "#         # metric_clean = model.score.output.save()\n",
    "#     hidden_states_clean = {k : v.value for k, v in hidden_states_clean.items()}\n",
    "#     # print(\"metric clean reward: -5.6 or 0.6: \",metric_clean)\n",
    "\n",
    "#     if patch is None:\n",
    "#         # print(f\"hidden state clean: {hidden_states_clean[submodule]}\")\n",
    "#         v = hidden_states_clean[submodule]\n",
    "#         # print(v)\n",
    "#         v_act = v.act.clone()\n",
    "#         v_res = v.res.clone()\n",
    "#         # print(f\"v_act shape: {v_act}\")\n",
    "#         # print(f\"v_res shape: {v_res.shape}\")\n",
    "#         for pos_ind, pos in enumerate(positions):\n",
    "#             v_act[pos_ind, pos:] = 0\n",
    "#             v_res[pos_ind, pos:] = 0\n",
    "#         hidden_states_patch = {\n",
    "#             # k : SparseAct(act=t.zeros_like(v.act), res=t.zeros_like(v.res)) for k, v in hidden_states_clean.items()\n",
    "#             k : SparseAct(act=v_act, res=v_res) for k, v in hidden_states_clean.items()\n",
    "#         }\n",
    "#         total_effect = None\n",
    "#     else:\n",
    "#         hidden_states_patch = {}\n",
    "#         with model.trace(patch, **tracer_kwargs), t.no_grad():\n",
    "#             for submodule in submodules:\n",
    "#                 dictionary = dictionaries[submodule]\n",
    "#                 x = submodule.output\n",
    "#                 if is_tuple[submodule]:\n",
    "#                     x = x[0]\n",
    "#                 f = dictionary.encode(x)\n",
    "#                 x_hat = dictionary.decode(f)\n",
    "#                 residual = x - x_hat\n",
    "#                 hidden_states_patch[submodule] = SparseAct(act=f.save(), res=residual.save())\n",
    "#             metric_patch = metric_fn(model, **metric_kwargs).save()\n",
    "#         total_effect = (metric_patch.value - metric_clean.value).detach()\n",
    "#         hidden_states_patch = {k : v.value for k, v in hidden_states_patch.items()}\n",
    "\n",
    "#     effects = {}\n",
    "#     deltas = {}\n",
    "#     grads = {}\n",
    "#     for submodule in submodules:\n",
    "#         dictionary = dictionaries[submodule]\n",
    "#         clean_state = hidden_states_clean[submodule]\n",
    "#         patch_state = hidden_states_patch[submodule]\n",
    "#         with model.trace(**tracer_kwargs) as tracer:\n",
    "#             metrics = []\n",
    "#             fs = []\n",
    "#             for step in range(steps):\n",
    "#                 alpha = step / steps\n",
    "#                 f = (1 - alpha) * clean_state + alpha * patch_state\n",
    "#                 f.act.retain_grad()\n",
    "#                 f.res.retain_grad()\n",
    "#                 fs.append(f)\n",
    "#                 with tracer.invoke(clean, scan=tracer_kwargs['scan']):\n",
    "#                     if is_tuple[submodule]:\n",
    "#                         submodule.output[0][:] = dictionary.decode(f.act) + f.res\n",
    "#                     else:\n",
    "#                         submodule.output = dictionary.decode(f.act) + f.res\n",
    "#                     # output_t = metric_fn(model, **metric_kwargs).save()\n",
    "#                     metrics.append(metric_fn(model, **metric_kwargs))\n",
    "\n",
    "#             metric = sum([m for m in metrics])\n",
    "#             mm = [m.detach().cpu().save() for m in metrics]\n",
    "#             metric.sum().backward(retain_graph=True)\n",
    "\n",
    "\n",
    "#         # print(\"Metrics\", output_t)\n",
    "#         # print(\"metric \", mm)\n",
    "#         mean_grad = sum([f.act.grad for f in fs]) / steps\n",
    "#         mean_residual_grad = sum([f.res.grad for f in fs]) / steps\n",
    "#         print('Out-Out next loop Memory Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "\n",
    "#         grad = SparseAct(act=mean_grad, res=mean_residual_grad)\n",
    "#         delta = (patch_state - clean_state).detach() if patch_state is not None else -clean_state.detach()\n",
    "#         # return grad, delta\n",
    "#         effect = grad @ delta\n",
    "\n",
    "#         # effects[submodule] = effect.detach().cpu()\n",
    "#         # deltas[submodule] = delta.detach().cpu()\n",
    "#         # grads[submodule] = grad.detach().cpu()\n",
    "#         # effects[submodule] = effect.act.detach().cpu()\n",
    "#         # deltas[submodule] = delta.act.detach().cpu()\n",
    "#         # grads[submodule] = grad.act.detach().cpu()\n",
    "#     return effect.act.detach().cpu()\n",
    "        \n",
    "#     return EffectOut(effects, deltas, grads, total_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  198,   198, 20490,    25,  1374,   466,   345,   795,  1350, 26413,\n",
       "           1637,    30,   198,   198, 48902,    25,   314,  1101,  7787,   326,\n",
       "            338,   407,   703,   340,  2499,    11,   460,   345,  4727,   517,\n",
       "             30]]),\n",
       " tensor([[  198,   198, 20490,    25,  1374,   466,   345,   795,  1350, 26413,\n",
       "           1637,    30,   198,   198, 48902,    25,   383,   749,  2219,   835,\n",
       "            284,   795,  1350, 26413,  1637,   318,   284,   625,  5219,   262,\n",
       "           1597,  3739,   290,   739,  5219,   262,  9307,   284,   787,   262,\n",
       "           1597,  1283,   517, 17967,   621,   340,  1682,   318,    13,   220,\n",
       "           1114,  1672,    11,   530,   743, 27807,  1958,  3331,  6299,   284,\n",
       "            787,   340,  1283,   588,   262,  1597,   447,   247,    82,  4200,\n",
       "            423,   587,   922,   290,   326,   262,  1597,  2125,   447,   247,\n",
       "             83,  2491,   257, 11807,    11,   523,   326,   484,   460,  5713,\n",
       "          10021,   284,  1663,   262,  1597,    13,   220,   887,   428,   318,\n",
       "            691,   530,  1672,    11,   290,   795,  1350,  3019,  1732,   460,\n",
       "            635,   307,  5281,   503,   832, 37388,   393,   584,  1724,    13]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_of_chosen_rejection_difference = torch.load(\"rm_save_files/index_of_chosen_rejection_difference.pt\")\n",
    "\n",
    "tokens_chosen = tokenizer(one_batch[\"chosen\"], padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "tokens_rejected = tokenizer(one_batch[\"rejected\"], padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "tokens_chosen, tokens_rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Search: Attribution Patching (AP) w/ Zero-Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Memory Allocated: 12.2 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token size:  torch.Size([1, 31])\n",
      "Out-Out next loop Memory Allocated: 25.2 GB\n",
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 120])\n",
      "Out-Out next loop Memory Allocated: 28.6 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/15887 [00:07<34:25:49,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 228])\n",
      "Out-Out next loop Memory Allocated: 32.7 GB\n",
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 341])\n",
      "Out-Out next loop Memory Allocated: 37.3 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/15887 [00:15<34:04:33,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 216])\n",
      "Out-Out next loop Memory Allocated: 32.3 GB\n",
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 268])\n",
      "Out-Out next loop Memory Allocated: 34.3 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/15887 [00:22<33:12:57,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 37])\n",
      "Out-Out next loop Memory Allocated: 25.4 GB\n",
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 148])\n",
      "Out-Out next loop Memory Allocated: 29.5 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/15887 [00:30<33:23:33,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 204])\n",
      "Out-Out next loop Memory Allocated: 31.8 GB\n",
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 207])\n",
      "Out-Out next loop Memory Allocated: 31.8 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/15887 [00:38<33:35:26,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 101])\n",
      "Out-Out next loop Memory Allocated: 27.7 GB\n",
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 90])\n",
      "Out-Out next loop Memory Allocated: 27.4 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/15887 [00:45<33:25:18,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 204])\n",
      "Out-Out next loop Memory Allocated: 31.8 GB\n",
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 214])\n",
      "Out-Out next loop Memory Allocated: 32.1 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/15887 [00:53<33:33:40,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 119])\n",
      "Out-Out next loop Memory Allocated: 28.4 GB\n",
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 194])\n",
      "Out-Out next loop Memory Allocated: 31.3 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/15887 [01:01<34:35:20,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 126])\n",
      "Out-Out next loop Memory Allocated: 28.7 GB\n",
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 166])\n",
      "Out-Out next loop Memory Allocated: 30.2 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/15887 [01:09<34:22:57,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be back to normal Memory Allocated: 24.1 GB\n",
      "token size:  torch.Size([1, 551])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/15887 [01:12<35:24:15,  8.03s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacty of 44.35 GiB of which 15.75 MiB is free. Process 3036166 has 44.33 GiB memory in use. Of the allocated memory 43.30 GiB is allocated by PyTorch, and 723.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# tokens = tokenizer(batch[\"chosen\"], padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# tokens = tokenizer(batch[\"rejected\"], padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken size: \u001b[39m\u001b[38;5;124m\"\u001b[39m, tokens\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 22\u001b[0m effects \u001b[38;5;241m=\u001b[39m \u001b[43mpatching_effect_two\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubmodules\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msubmodules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdictionaries\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdictionaries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtracer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_reward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# all_effects_per_feature[:, batch_ind] = effects.sum(0).sum(0)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# print(text_ind)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m all_effects_per_feature[:, batch_ind\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m text_ind] \u001b[38;5;241m=\u001b[39m effects\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/sae-rm/interp_utils.py:588\u001b[0m, in \u001b[0;36mpatching_effect_two\u001b[0;34m(clean, patch, model, submodules, dictionaries, metric_fn, tracer_kwargs, positions, steps, metric_kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m clean_state \u001b[38;5;241m=\u001b[39m hidden_states_clean[submodule]\n\u001b[1;32m    587\u001b[0m patch_state \u001b[38;5;241m=\u001b[39m hidden_states_patch[submodule]\n\u001b[0;32m--> 588\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtracer_kwargs) \u001b[38;5;28;01mas\u001b[39;00m tracer:\n\u001b[1;32m    589\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    590\u001b[0m     fs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/nnsight/contexts/Runner.py:49\u001b[0m, in \u001b[0;36mRunner.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/nnsight/contexts/Tracer.py:69\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[0;32m---> 69\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterleave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mtracing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/nnsight/models/NNsightModel.py:245\u001b[0m, in \u001b[0;36mNNsight.interleave\u001b[0;34m(self, fn, intervention_graph, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m inputs, total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_inputs(\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m    243\u001b[0m intervention_handler \u001b[38;5;241m=\u001b[39m InterventionHandler(intervention_graph, total_batch_size)\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m HookHandler(\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model,\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mlist\u001b[39m(intervention_graph\u001b[38;5;241m.\u001b[39margument_node_names\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[1;32m    248\u001b[0m     input_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m activations, module_path: intervene(\n\u001b[1;32m    249\u001b[0m         activations, module_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, intervention_handler\n\u001b[1;32m    250\u001b[0m     ),\n\u001b[1;32m    251\u001b[0m     output_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m activations, module_path: intervene(\n\u001b[1;32m    252\u001b[0m         activations, module_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m, intervention_handler\n\u001b[1;32m    253\u001b[0m     ),\n\u001b[1;32m    254\u001b[0m ):\n\u001b[1;32m    255\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    257\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/nnsight/intervention.py:450\u001b[0m, in \u001b[0;36mHookHandler.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    447\u001b[0m     handle\u001b[38;5;241m.\u001b[39mremove()\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/nnsight/models/NNsightModel.py:255\u001b[0m, in \u001b[0;36mNNsight.interleave\u001b[0;34m(self, fn, intervention_graph, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m intervention_handler \u001b[38;5;241m=\u001b[39m InterventionHandler(intervention_graph, total_batch_size)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m HookHandler(\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model,\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mlist\u001b[39m(intervention_graph\u001b[38;5;241m.\u001b[39margument_node_names\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m     ),\n\u001b[1;32m    254\u001b[0m ):\n\u001b[0;32m--> 255\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# gc.collect()\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# torch.cuda.empty_cache()\u001b[39;00m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/nnsight/models/mixins/Generation.py:21\u001b[0m, in \u001b[0;36mGenerationMixin._execute\u001b[0;34m(self, prepared_inputs, generate, *args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generate:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_generate(prepared_inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/nnsight/models/LanguageModel.py:291\u001b[0m, in \u001b[0;36mLanguageModel._execute_forward\u001b[0;34m(self, prepared_inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, prepared_inputs: Any, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    289\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepared_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:1248\u001b[0m, in \u001b[0;36mGPTJForSequenceClassification.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1248\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1262\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:950\u001b[0m, in \u001b[0;36mGPTJModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    939\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    940\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    941\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m         output_attentions,\n\u001b[1;32m    948\u001b[0m     )\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 950\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:578\u001b[0m, in \u001b[0;36mGPTJBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, position_ids, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    576\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    577\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 578\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    588\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:276\u001b[0m, in \u001b[0;36mGPTJAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, position_ids, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    273\u001b[0m     present \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# compute self-attention: V x Softmax(QK^T)\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    279\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(attn_output)\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:178\u001b[0m, in \u001b[0;36mGPTJAttention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    175\u001b[0m query \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    176\u001b[0m key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 178\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacty of 44.35 GiB of which 15.75 MiB is free. Process 3036166 has 44.33 GiB memory in use. Of the allocated memory 43.30 GiB is allocated by PyTorch, and 723.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from interp_utils import patching_effect_two\n",
    "import gc\n",
    "# import torch\n",
    "gc.collect()\n",
    "tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "def get_reward(model):\n",
    "    return model.output.logits[:, 0]\n",
    "torch.cuda.empty_cache()\n",
    "print('Original Memory Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "\n",
    "num_features = sae.encoder.weight.shape[0]\n",
    "num_datapoints = len(hh)*2\n",
    "all_effects_per_feature = torch.zeros((num_features, num_datapoints))\n",
    "for batch_ind, batch in enumerate(tqdm(hh)):\n",
    "    pos = [index_of_chosen_rejection_difference[batch_ind].item()]\n",
    "\n",
    "    for text_ind, text_key in enumerate([\"chosen\", \"rejected\"]):\n",
    "        tokens = tokenizer(batch[text_key], padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        # tokens = tokenizer(batch[\"chosen\"], padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        # tokens = tokenizer(batch[\"rejected\"], padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        print(\"token size: \", tokens.shape)\n",
    "        effects = patching_effect_two(\n",
    "            tokens.to(device),\n",
    "            None,\n",
    "            model,\n",
    "            submodules = submodules,\n",
    "            dictionaries = dictionaries,\n",
    "            tracer_kwargs=tracer_kwargs,\n",
    "            positions = pos,\n",
    "            metric_fn = get_reward,\n",
    "            steps = 4,\n",
    "        )\n",
    "\n",
    "        # all_effects_per_feature[:, batch_ind] = effects.sum(0).sum(0)\n",
    "        # print(text_ind)\n",
    "        all_effects_per_feature[:, batch_ind*2 + text_ind] = effects.sum(0).sum(0)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print('Should be back to normal Memory Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "            # break\n",
    "    # break\n",
    "\n",
    "# # tokens = tokenizer(one_batch[\"chosen\"], padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "# tokens = tokenizer(one_batch[\"rejected\"], padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "# pos = [index_of_chosen_rejection_difference[0]]\n",
    "# # effects, _, _, total_effect = patching_effect_two(\n",
    "# # grad, delta = patching_effect_two(\n",
    "# effects = patching_effect_two(\n",
    "#     tokens.to(device),\n",
    "#     None,\n",
    "#     model,\n",
    "#     submodules = submodules,\n",
    "#     dictionaries = dictionaries,\n",
    "#     tracer_kwargs=tracer_kwargs,\n",
    "#     positions = pos,\n",
    "#     metric_fn = get_reward,\n",
    "#     steps = 15,\n",
    "# )\n",
    "# torch.cuda.empty_cache()\n",
    "# for submodule in submodules:\n",
    "#     effects[submodule] = effects[submodule].act\n",
    "# module_effect = effects[module]\n",
    "# # Sum over all datapoints & positions\n",
    "# top_val, top_features = module_effect.sum(0).sum(0).topk(top_k_features)\n",
    "# top_threshold = 0.9\n",
    "# top_thresh_effect_features = ((top_val.cumsum(0) / top_val.sum()) > top_threshold).nonzero()[0][0].item()\n",
    "# top_features = top_features[:top_thresh_effect_features]\n",
    "# top_features = top_features[:3]\n",
    "# print(\"90\\% of effect is in top\", top_thresh_effect_features, \"features\")\n",
    "# effects.sum(0).sum(0).topk(3, largest=False), effects.sum(0).sum(0).topk(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'),\n",
       " device(type='cpu'),\n",
       " device(type='cpu'),\n",
       " device(type='cuda', index=0))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import gc\n",
    "# # import torch\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "effects.device, all_effects_per_feature.device, tokens.device, model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.topk(\n",
       " values=tensor([-59.5883, -13.4602, -12.4965,  -9.4696,  -7.4027]),\n",
       " indices=tensor([ 6223,  7070,  7251, 11611,  6855])),\n",
       " torch.return_types.topk(\n",
       " values=tensor([5.5858, 2.3210, 2.2476, 1.9482, 1.8790]),\n",
       " indices=tensor([16464,  9756, 19548, 32237, 12232])),\n",
       " [\"\\n\\nHuman: How do you embezzle money?\\n\\nAssistant: I'm afraid that's not how it works, can you explain more?\"])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effects.sum(0).sum(0).topk(5, largest=False), effects.sum(0).sum(0).topk(5), one_batch[\"chosen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,  -2.2997,   2.6151,   3.2930,   3.3418,  -0.9398,\n",
      "         -4.1970,  -6.1131,   3.9386,  -0.6903,   0.1392,  -2.6519,  -1.9955,\n",
      "          0.5399,   5.2570, -22.9634]) tensor(6223)\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0878,  0.0000,  0.0121,  5.2373,  0.9116]) tensor(10962)\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 4.5416, 0.0000]) tensor(26170)\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 4.3362, 0.0000, 0.0000]) tensor(14706)\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         1.2949,  1.5315,  3.4218,  1.5339, -0.5842, -1.7533, -1.0133, -0.0715,\n",
      "         0.0534,  0.3870,  0.2764,  0.5397,  1.3656, -1.5149, -3.9612]) tensor(7070)\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 3.2566, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000]) tensor(91)\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 2.6310, 0.4871, 0.5024]) tensor(32670)\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000, -0.1935,  2.4452, -0.0316,  0.1003,\n",
      "         0.0000,  0.0152,  0.0000,  0.0000,  0.0000,  0.0243,  0.1142]) tensor(8109)\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        2.0464, 0.0000, 0.0000, 0.0000]) tensor(3349)\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        2.0353, 0.0000, 0.0000, 0.0000]) tensor(31754)\n"
     ]
    }
   ],
   "source": [
    "ind = effects.sum(0).max(0).values.topk(10).indices\n",
    "for i in range(10):\n",
    "    print(effects.sum(0)[:, ind[i]], ind[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.topk(\n",
       " values=tensor([-11.5858,  -4.2712,  -2.0474,  -1.9267,  -1.8461]),\n",
       " indices=tensor([27334,  9970, 21261, 27116, 28340])),\n",
       " torch.return_types.topk(\n",
       " values=tensor([12.7521, 11.6801,  2.1554,  2.0833,  1.8991]),\n",
       " indices=tensor([ 7070,  6223,  8109, 14706,    91])),\n",
       " [\"\\n\\nHuman: How do you embezzle money?\\n\\nAssistant: I'm afraid that's not how it works, can you explain more?\"])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effects.sum(0).sum(0).topk(5, largest=False), effects.sum(0).sum(0).topk(5), one_batch[\"chosen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.topk(\n",
       " values=tensor([-4.0716, -3.3590, -1.4339, -1.3321, -0.8645]),\n",
       " indices=tensor([ 6223, 27334, 28340,  9970, 16493])),\n",
       " torch.return_types.topk(\n",
       " values=tensor([2.5129, 0.9682, 0.8176, 0.5162, 0.4547]),\n",
       " indices=tensor([14706, 32670, 10962,  3349, 16162])),\n",
       " [\"\\n\\nHuman: How do you embezzle money?\\n\\nAssistant: I'm afraid that's not how it works, can you explain more?\"])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7251\n",
    "# effects.sum(0)[:, 6223]\n",
    "# top_neg_ind = effects.sum(0).sum(0).topk(5, largest=False).indices\n",
    "# top_pos_ind = effects.sum(0).sum(0).topk(5).indices\n",
    "# for ind in top_neg_ind:\n",
    "#     print(ind, effects.sum(0)[:, ind])\n",
    "# for ind in top_pos_ind:\n",
    "#     print(ind, effects.sum(0)[:, ind])\n",
    "# print(one_batch[\"chosen\"])\n",
    "effects.sum(0).sum(0).topk(5, largest=False), effects.sum(0).sum(0).topk(5), one_batch[\"chosen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 202])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(one_batch[\"chosen\"], padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'LanguageModel' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token_len \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m      8\u001b[0m     start \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mtoken_len\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     total_times[token_len] \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# print(\"Token length\", token_len, \"Time\", total_times[token_len])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# with model.trace(one_batch[\"rejected\"], **tracer_kwargs):\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     output = model.output.save()\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'LanguageModel' object is not callable"
     ]
    }
   ],
   "source": [
    "#import a code timing function\n",
    "from time import time\n",
    "\n",
    "\n",
    "tokens = tokenizer(one_batch[\"chosen\"], padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "total_times = torch.zeros(tokens.shape[-1])\n",
    "for token_len in range(tokens.shape[-1]):\n",
    "    start = time()\n",
    "    model(tokens[:, :token_len].to(device))\n",
    "    total_times[token_len] = time() - start\n",
    "    # print(\"Token length\", token_len, \"Time\", total_times[token_len])\n",
    "# with model.trace(one_batch[\"rejected\"], **tracer_kwargs):\n",
    "#     output = model.output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([-9.0808, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000], device='cuda:0'),\n",
       "indices=tensor([7251,    3,    7,   18,   19,    2,    4,   13,   20,   11],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effects[module].act.reshape(-1).topk(10, largest=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n\\nHuman: What are some cuss words in english?\\n\\nAssistant: Here’s an incomplete list.\\n\\nAss, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cum, cumbucket, fucknugget, butthole, poop, jackass, cocksucker, asshole, goddamn, piss, sperm, blow, wank, jism, cum-sucking, masturbate, faggot, queer, jizz, jizz-licking, prostitute, slut, cheater, fornicator, floozy, wetback, Mexican, Hispanic, sodomite, midget, mama’s boy, faggot, pervert, queer, scumbag, bitch,\\n\\nHuman: What's your favorite one?\\n\\nAssistant: I haven't even thought about it.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_batch[\"chosen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/sae-rm/logan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/root/sae-rm/logan/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:18<00:00,  6.10s/it]\n",
      "  0%|          | 0/768 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index is out of bounds for dimension with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token_len \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])):\n\u001b[1;32m     17\u001b[0m         start \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m---> 18\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mtoken_len\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         total_times[token_len] \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:1248\u001b[0m, in \u001b[0;36mGPTJForSequenceClassification.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1248\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1262\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:848\u001b[0m, in \u001b[0;36mGPTJModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 848\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn_if_padding_and_no_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    850\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/sae-rm/logan/lib/python3.10/site-packages/transformers/modeling_utils.py:4457\u001b[0m, in \u001b[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   4454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   4456\u001b[0m \u001b[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001b[39;00m\n\u001b[0;32m-> 4457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[1;32m   4458\u001b[0m     warn_string \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4459\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/troubleshooting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#incorrect-output-when-padding-tokens-arent-masked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4462\u001b[0m     )\n\u001b[1;32m   4464\u001b[0m     \u001b[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001b[39;00m\n\u001b[1;32m   4465\u001b[0m     \u001b[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index is out of bounds for dimension with size 0"
     ]
    }
   ],
   "source": [
    "#import a code timing function\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"reciprocate/dahoas-gptj-rm-static\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "really_long_text = \"epsum factorial non deposit quid pro quo hic escorol. Olypian quarrels et gorilla congolium sic ad nauseum. Souvlaki ignitus carborundum e pluribus unum. Defacto lingo est igpay atinlay. Marquee selectus non provisio incongruous feline nolo contendre. Gratuitous octopus niacin, sodium glutimate. Quote meon an estimate et non interruptus stadium. Sic tempus fugit esperanto hiccup estrogen. Glorious baklava ex librus hup hey ad infinitum. Non sequitur condominium facile et geranium incognito. Epsum factorial non deposit quid pro quo hic escorol. Marquee selectus non provisio incongruous feline nolo contendre Olypian quarrels et gorilla congolium sic ad nauseum. Souvlaki ignitus carborundum e pluribus unum. Defacto lingo est igpay atinlay. Marquee selectus non provisio incongruous feline nolo contendre. Gratuitous octopus niacin, sodium glutimate. Quote meon an estimate et non interruptus stadium. Sic tempus fugit esperanto hiccup estrogen. Glorious baklava ex librus hup hey ad infinitum. Non sequitur condominium facile et geranium incognito. Epsum factorial non deposit quid pro quo hic escorol. Marquee selectus non provisio incongruous feline nolo contendre Olypian quarrels et gorilla congolium sic ad nauseum. Souvlaki ignitus carborundum e pluribus unum. Defacto lingo est igpay atinlay. Marquee selectus non provisio incongruous feline nolo contendre. Gratuitous octopus niacin, sodium glutimate. Quote meon an estimate et non interruptus stadium. Sic tempus fugit esperanto hiccup estrogen. Glorious baklava ex librus hup hey ad infinitum. Non sequitur condominium facile et geranium incognito. Epsum factorial non deposit quid pro quo hic escorol. Marquee selectus non provisio incongruous feline nolo contendre Olypian quarrels et gorilla congolium sic ad nauseum. Souvlaki ignitus carborundum e pluribus unum. Defacto lingo est igpay atinlay. Marquee selectus non provisio incongruous feline nolo contendre. Gratuitous octopus niacin, sodium glutimate. Quote meon an estimate et non interruptus stadium. Sic tempus fugit esperanto hiccup estrogen. Glorious baklava ex librus hup hey ad infinitum. Non sequitur condominium facile et geranium incognito. Epsum factorial non deposit quid pro quo hic escorol. Marquee selectus non provisio incongruous feline nolo contendre Olypian quarrels et gorilla congolium sic ad nauseum. Souvlaki ignitus carborundum e pluribus unum. Defacto lingo est igpay atinlay. Marquee selectus non provisio incongruous feline nolo contendre. Gratuitous octopus niacin, sodium glutimate. Quote meon\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokens = tokenizer(really_long_text, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "total_times = torch.zeros(tokens.shape[-1])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for token_len in tqdm(range(1, tokens.shape[-1])):\n",
    "        start = time()\n",
    "        model(tokens[:, :token_len].to(device))\n",
    "        total_times[token_len] = time() - start\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(total_times.numpy())\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"Inference Time vs. Token Length\")\n",
    "# tokens.shape, token_len\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 767/767 [04:00<00:00,  3.18it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkEAAAHHCAYAAAC4BYz1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABS+ElEQVR4nO3deVxU5f4H8M8szAzbDPsOAq64oqBIalZSVGZ5W6SyXCr7mVqaeUsrMesm3W5a3Ztp2VVbb9rm7WZq7qmRO+4LKogLqwgDAwww8/z+MCZHQBYHBuZ83q/XvF7OOc85831mJubTc55zjkwIIUBEREQkMXJ7F0BERERkDwxBREREJEkMQURERCRJDEFEREQkSQxBREREJEkMQURERCRJDEFEREQkSQxBREREJEkMQURERCRJDEEkaaWlpXjqqacQEBAAmUyGadOm2bukNm/Lli2QyWTYsmWLvUtp95YvXw6ZTIY9e/bYu5R27bXXXoNMJkNBQYG9S6F2hiGI2rUb/RGZN28eli9fjmeeeQaff/45Hn/8cRtX2D6MGzcOMpmswce4cePsXard1XznGnqEh4fbu9QmyczMhEwmwzvvvGPvUuo1b948rFq1yt5lkANR2rsAInvatGkTBg4ciDlz5ti7FLv6v//7PyQkJFieZ2RkIDk5GU8//TSGDBliWd6xY0fExcWhvLwcKpXKHqXa3c0334zPP//catlTTz2FAQMG4Omnn7Ysc3Nza+3SHN68efPw4IMPYuTIkfYuhRwEQxBJWl5eHrp3726z/ZnNZlRWVkKj0dhsn60hPj4e8fHxlud79uxBcnIy4uPj8dhjj9Vq3976Z0uRkZGIjIy0WjZx4kRERkbW+V4RUdvFw2HkcMaNGwc3NzdcuHABI0eOhJubG3x9fTFjxgyYTCYAf85rycjIwOrVqy2HMDIzMwEARqMRc+bMQadOnaBWqxEaGooXX3wRRqPR6rVkMhmmTJmCL7/8Ej169IBarcbatWsBABcuXMATTzwBf39/qNVq9OjRA0uXLrXavqaOlStX4s0330RISAg0Gg2GDRuGU6dO1erbzp07cffdd8PT0xOurq7o3bs33n//fas2x48fx4MPPggvLy9oNBrExsbixx9/tNXbW+ecoFtuuQU9e/bEwYMHMXToULi4uKBTp0749ttvAQBbt25FXFwcnJ2d0bVrV2zYsKHWfhvzftWlZ8+euPXWW2stN5vNCA4OxoMPPmhZ9vXXXyMmJgbu7u7QarXo1atXrffPVvbv34+77roLWq0Wbm5uGDZsGH7//fcGt7t8+TIGDBiAkJAQnDhxAkDTv4+rVq1Cz549Le9jzXfSFlqili1btiA2NhYajQYdO3bERx99ZJnnc/X+DAYDPv3003oPzxYVFWHcuHHw8PCATqfD+PHjUVZWZrO+k+PhSBA5JJPJhMTERMTFxeGdd97Bhg0bMH/+fHTs2BHPPPMMoqKi8Pnnn+P5559HSEgIXnjhBQCAr68vzGYz7r33Xmzfvh1PP/00oqKicOjQIbz77rs4efJkrTkJmzZtwsqVKzFlyhT4+PggPDwcubm5GDhwoOWHwNfXF2vWrMGTTz4JvV5fawL2W2+9BblcjhkzZqC4uBhvv/02Ro8ejZ07d1rarF+/Hvfccw8CAwMxdepUBAQE4NixY/jpp58wdepUAMCRI0cwaNAgBAcHY+bMmXB1dcXKlSsxcuRIfPfdd/jLX/7SYu/55cuXcc899+Dhhx/GQw89hEWLFuHhhx/Gl19+iWnTpmHixIl49NFH8Y9//AMPPvggzp07B3d3dwBo8vt1taSkJLz22mvIyclBQECAZfn27dtx8eJFPPzww5b375FHHsGwYcPw97//HQBw7Ngx7Nixw/L+2cqRI0cwZMgQaLVavPjii3BycsJHH32EW265xRII61JQUIDbb78dhYWF2Lp1Kzp27Njk7+P27dvx/fffY9KkSXB3d8c///lPPPDAA8jKyoK3t/cN9aslatm/fz/uvPNOBAYGYu7cuTCZTHj99dfh6+trta/PP/+81mHHjh07WrUZNWoUIiIikJKSgn379uGTTz6Bn5+f5fMmqkUQtWPLli0TAMTu3bsty8aOHSsAiNdff92qbd++fUVMTIzVsg4dOojhw4dbLfv888+FXC4X27Zts1q+ePFiAUDs2LHDsgyAkMvl4siRI1Ztn3zySREYGCgKCgqslj/88MNCp9OJsrIyIYQQmzdvFgBEVFSUMBqNlnbvv/++ACAOHTokhBCiurpaREREiA4dOojLly9b7dNsNlv+PWzYMNGrVy9RUVFhtf6mm24SnTt3Fo21e/duAUAsW7as1rqamjdv3mxZNnToUAFAfPXVV5Zlx48ft7w/v//+u2X5unXrau27se9XXU6cOCEAiH/9619WyydNmiTc3Nws206dOlVotVpRXV3dmLegSVxdXcXYsWMtz0eOHClUKpU4ffq0ZdnFixeFu7u7uPnmmy3Lrv7+Zmdnix49eojIyEiRmZlpadPU76NKpRKnTp2yLDtw4ECd78+1MjIyBADxj3/8o942LVHLiBEjhIuLi7hw4YJlWXp6ulAqleLan6hr3+cac+bMEQDEE088YbX8L3/5i/D29r5uv0naeDiMHNbEiROtng8ZMgRnzpxpcLtvvvkGUVFR6NatGwoKCiyP2267DQCwefNmq/ZDhw61mlckhMB3332HESNGQAhhtY/ExEQUFxdj3759VvsYP3681UTjmsnINfXu378fGRkZmDZtGjw8PKy2rTlkUFhYiE2bNmHUqFEoKSmxvOalS5eQmJiI9PR0XLhwocH+N5ebm5tl1AUAunbtCg8PD0RFRVmNfNT8u6ZvzXm/rtalSxdER0djxYoVlmUmkwnffvstRowYAWdnZwCAh4cHDAYD1q9fb9N+X8tkMuGXX37ByJEjreYOBQYG4tFHH8X27duh1+uttjl//jyGDh2Kqqoq/Prrr+jQoYNlXVO/jwkJCVYjJL1794ZWq23Ud78htq7FZDJhw4YNGDlyJIKCgiztOnXqhLvuuqvJ9dX13/ylS5dqvd9ENXg4jBySRqOpNZzu6emJy5cvN7hteno6jh07Vmv7Gnl5eVbPIyIirJ7n5+ejqKgIH3/8MT7++ONG7SMsLKxWrQAs9Z4+fRrAlfkv9Tl16hSEEJg9ezZmz55d7+sGBwfXu48bERISYjWHAwB0Oh1CQ0NrLQP+7Ftz3q9rJSUl4eWXX8aFCxcQHByMLVu2IC8vD0lJSZY2kyZNwsqVK3HXXXchODgYd9xxB0aNGoU777yzyX29nvz8fJSVlaFr16611kVFRcFsNuPcuXPo0aOHZfnjjz8OpVKJY8eOWR3SA5r+fbz2uwQ0/rvfEFvXkpeXh/LycnTq1KlWu7qWNeR6/x1ptdom748cH0MQOSSFQtHsbc1mM3r16oUFCxbUuf7aH/WakYartweAxx57DGPHjq1zH71797Z6Xl+9QohG1Xz1686YMQOJiYl1tmnOD0tj1deHhvrWnPfrWklJSZg1axa++eYbTJs2DStXroROp7MKOH5+fkhLS8O6deuwZs0arFmzBsuWLcOYMWPw6aefNti/lnT//ffjs88+w/vvv4+UlBSrdU39Ptriu1SftlRLXVr79aj9YwgiukbHjh1x4MABDBs2rNbIRmP4+vrC3d0dJpPJ6to7N1oTABw+fLjefdYcenFycrLZ67YGW7xfERERGDBgAFasWIEpU6bg+++/x8iRI6FWq63aqVQqjBgxAiNGjIDZbMakSZPw0UcfYfbs2TYLiL6+vnBxcbGc2XW148ePQy6X1woLzz77LDp16oTk5GTodDrMnDnTsu5Gv4+2ZOta/Pz8oNFo6jwTsq5l9u4/OR7OCSK6xqhRo3DhwgUsWbKk1rry8nIYDIbrbq9QKPDAAw/gu+++w+HDh2utz8/Pb3JN/fr1Q0REBN577z0UFRVZrav5v1w/Pz/ccsst+Oijj5CdnW2T120Ntnq/kpKS8Pvvv2Pp0qUoKCiwOhQGAJcuXbJ6LpfLLSNMNad3V1VV4fjx43W+f42lUChwxx134L///a/lkgvAlTPgvvrqKwwePLjOQzOzZ8/GjBkzMGvWLCxatMiy/Ea/j7Zk61oUCgUSEhKwatUqXLx40bL81KlTWLNmTa32rq6utb7/RDeCI0FE13j88cexcuVKTJw4EZs3b8agQYNgMplw/PhxrFy5EuvWrUNsbOx19/HWW29h8+bNiIuLw4QJE9C9e3cUFhZi37592LBhAwoLC5tUk1wux6JFizBixAhER0dj/PjxCAwMxPHjx3HkyBGsW7cOALBw4UIMHjwYvXr1woQJExAZGYnc3Fykpqbi/PnzOHDgQLPfl5Zki/dr1KhRmDFjBmbMmAEvL69ao0pPPfUUCgsLcdtttyEkJARnz57Fv/71L0RHRyMqKgrAlWsVRUVFYezYsVi+fHmz+/O3v/0N69evx+DBgzFp0iQolUp89NFHMBqNePvtt+vd7h//+AeKi4sxefJkuLu747HHHrPJ97EpNm7ciIqKilrLR44c2SK1vPbaa/jll18waNAgPPPMMzCZTPjggw/Qs2dPpKWlWbWNiYnBhg0bsGDBAgQFBSEiIqLeyw0QNQZDENE15HI5Vq1ahXfffRefffYZfvjhB7i4uCAyMhJTp05Fly5dGtyHv78/du3ahddffx3ff/89PvzwQ3h7e6NHjx7NvmZJYmIiNm/ejLlz52L+/Pkwm83o2LEjJkyYYGnTvXt37NmzB3PnzsXy5ctx6dIl+Pn5oW/fvkhOTm7W67YGW7xfISEhuOmmm7Bjxw489dRTcHJyslr/2GOP4eOPP8aHH36IoqIiBAQEWK4xJJfbdlC8R48e2LZtG2bNmoWUlBSYzWbExcXhiy++aPBHe/HixSgtLcX48ePh7u6O++6774a/j02xdu3aOi9oGB4ejp49e9q8lpiYGKxZswYzZszA7NmzERoaitdffx3Hjh3D8ePHrdouWLAATz/9NF599VWUl5dj7NixDEF0Q2SCM8aIiKiNGTlyJI4cOYL09HR7l0IOjHOCiIjIrsrLy62ep6en4+eff8Ytt9xin4JIMjgSREREdhUYGIhx48YhMjISZ8+exaJFi2A0GrF//3507tzZ3uWRA+OcICIisqs777wT//nPf5CTkwO1Wo34+HjMmzePAYhaHEeCiIiISJI4J4iIiIgkiSGIiIiIJElyc4LMZjMuXrwId3d3XoKdiIionRBCoKSkBEFBQTa7tpfkQtDFixdr3beHiIiI2odz584hJCTEJvuSXAhyd3cHcOVNrOv+PURERNT26PV6hIaGWn7HbUFyIajmEJhWq2UIIiIiamdsOZWFE6OJiIhIkhiCiIiISJIYgoiIiEiSGIKIiIhIkhiCiIiISJIYgoiIiEiSGIKIiIhIkhiCiIiISJIYgoiIiEiSGIKIiIhIkhiCiIiISJIYgoiIiEiSGIJsrLzSZO8SiIiIqBEYgmxo4eZTiEpei43Hcu1dChERETWAIciG/rHuBABg5veH7FwJERERNYQhqAUIYe8KiIiIqCEMQURERCRJDEEtQCazdwVERETUEIYgIiIikiS7hqBff/0VI0aMQFBQEGQyGVatWtXgNlu2bEG/fv2gVqvRqVMnLF++vMXrJCIiIsdj1xBkMBjQp08fLFy4sFHtMzIyMHz4cNx6661IS0vDtGnT8NRTT2HdunUtXCkRERE5GqU9X/yuu+7CXXfd1ej2ixcvRkREBObPnw8AiIqKwvbt2/Huu+8iMTGxpcokIiIiB9Su5gSlpqYiISHBalliYiJSU1Pr3cZoNEKv11s9iIiIiNpVCMrJyYG/v7/VMn9/f+j1epSXl9e5TUpKCnQ6neURGhraGqUSERFRG9euQlBzzJo1C8XFxZbHuXPnWvw1ebFEIiKits+uc4KaKiAgALm51vflys3NhVarhbOzc53bqNVqqNXq1iiPiIiI2pF2NRIUHx+PjRs3Wi1bv3494uPj7VQRERERtVd2DUGlpaVIS0tDWloagCunwKelpSErKwvAlUNZY8aMsbSfOHEizpw5gxdffBHHjx/Hhx9+iJUrV+L555+3R/lERETUjtk1BO3Zswd9+/ZF3759AQDTp09H3759kZycDADIzs62BCIAiIiIwOrVq7F+/Xr06dMH8+fPxyeffMLT44mIiKjJ7Don6JZbboG4ziziuq4Gfcstt2D//v0tWBURERFJQbuaE0RERERkKwxBREREJEkMQURERCRJDEFEREQkSQxBREREJEkMQURERCRJDEFEREQkSQxBREREJEkMQURERCRJDEFEREQkSQxBREREJEkMQURERCRJDEFEREQkSQxBREREJEkMQURERCRJDEFEREQkSQxBREREJEkMQURERCRJDEFEREQkSQxBREREJEkMQURERCRJDEFEREQkSQxBREREJEkMQS1C2LsAIiIiagBDEBEREUkSQxARERFJEkMQERERSRJDEBEREUkSQxARERFJEkMQERERSRJDEBEREUkSQxARERFJEkMQERERSRJDEBEREUkSQxARERFJEkMQERERSRJDEBEREUkSQxARERFJEkMQERERSRJDEBEREUkSQxARERFJEkMQERERSRJDEBEREUkSQxARERFJEkMQERERSRJDEBEREUkSQxARERFJEkMQERERSRJDEBEREUkSQxARERFJEkMQERERSRJDEBEREUkSQ1ALEMLeFRAREVFDGIKIiIhIkuweghYuXIjw8HBoNBrExcVh165d123/3nvvoWvXrnB2dkZoaCief/55VFRUtFK1RERE5CjsGoJWrFiB6dOnY86cOdi3bx/69OmDxMRE5OXl1dn+q6++wsyZMzFnzhwcO3YM//73v7FixQq8/PLLrVw5ERERtXd2DUELFizAhAkTMH78eHTv3h2LFy+Gi4sLli5dWmf73377DYMGDcKjjz6K8PBw3HHHHXjkkUcaHD0iIiIiupbdQlBlZSX27t2LhISEP4uRy5GQkIDU1NQ6t7npppuwd+9eS+g5c+YMfv75Z9x99931vo7RaIRer7d6EBERESnt9cIFBQUwmUzw9/e3Wu7v74/jx4/Xuc2jjz6KgoICDB48GEIIVFdXY+LEidc9HJaSkoK5c+fatHYiIiJq/+w+MboptmzZgnnz5uHDDz/Evn378P3332P16tV444036t1m1qxZKC4utjzOnTvXihUTERFRW2W3kSAfHx8oFArk5uZaLc/NzUVAQECd28yePRuPP/44nnrqKQBAr169YDAY8PTTT+OVV16BXF4706nVaqjVatt3gIiIiNo1u40EqVQqxMTEYOPGjZZlZrMZGzduRHx8fJ3blJWV1Qo6CoUCACB4hUIiIiJqAruNBAHA9OnTMXbsWMTGxmLAgAF47733YDAYMH78eADAmDFjEBwcjJSUFADAiBEjsGDBAvTt2xdxcXE4deoUZs+ejREjRljCEBEREVFj2DUEJSUlIT8/H8nJycjJyUF0dDTWrl1rmSydlZVlNfLz6quvQiaT4dVXX8WFCxfg6+uLESNG4M0337RXF4iIiKidkgmJHUfS6/XQ6XQoLi6GVqu16b7DZ64GAHi7qrB39u023TcREZGUtcTvd7s6O4yIiIjIVhiCiIiISJIYgoiIiEiSGIKIiIhIkhiCiIiISJIYgoiIiEiSGIKIiIhIkhiCiIiISJIYgoiIiEiSGIKIiIhIkhiCiIiISJIYgoiIiEiSGIKIiIhIkhiCiIiISJIYgoiIiEiSGIKIiIhIkhiCWoCwdwFERETUIIYgIiIikiSGICIiIpIkhiAiIiKSJIYgIiIikiSGICIiIpIkhiAiIiKSJIYgIiIikiSGICIiIpIkhiAiIiKSJIYgIiIikiSGICIiIpIkhiAiIiKSJIYgIiIikiSGICIiIpIkhiAiIiKSJIYgIiIikiSGICIiIpIkhiAiIiKSJIYgIiIikiSGICIiIpIkhiAiIiKSJIYgIiIikiSGICIiIpIkhiAiIiKSJIYgIiIikiSGICIiIpIkhiAiIiKSJIYgIiIikiSGoBYghLB3CURERNQAhiAiIiKSJIYgIiIikiSGICIiIpIkhiAiIiKSJIYgIiIikiSGICIiIpIkhiAiIiKSJIYgIiIikiS7h6CFCxciPDwcGo0GcXFx2LVr13XbFxUVYfLkyQgMDIRarUaXLl3w888/t1K1RERE5CiU9nzxFStWYPr06Vi8eDHi4uLw3nvvITExESdOnICfn1+t9pWVlbj99tvh5+eHb7/9FsHBwTh79iw8PDxav3giIiJq1+waghYsWIAJEyZg/PjxAIDFixdj9erVWLp0KWbOnFmr/dKlS1FYWIjffvsNTk5OAIDw8PDWLJmIiIgchN0Oh1VWVmLv3r1ISEj4sxi5HAkJCUhNTa1zmx9//BHx8fGYPHky/P390bNnT8ybNw8mk6m1yiYiIiIHYbeRoIKCAphMJvj7+1st9/f3x/Hjx+vc5syZM9i0aRNGjx6Nn3/+GadOncKkSZNQVVWFOXPm1LmN0WiE0Wi0PNfr9bbrBBEREbVbdp8Y3RRmsxl+fn74+OOPERMTg6SkJLzyyitYvHhxvdukpKRAp9NZHqGhoa1YMREREbVVzQ5BVVVVOHfuHE6cOIHCwsImb+/j4wOFQoHc3Fyr5bm5uQgICKhzm8DAQHTp0gUKhcKyLCoqCjk5OaisrKxzm1mzZqG4uNjyOHfuXJNrJSIiIsfTpBBUUlKCRYsWYejQodBqtQgPD0dUVBR8fX3RoUMHTJgwAbt3727UvlQqFWJiYrBx40bLMrPZjI0bNyI+Pr7ObQYNGoRTp07BbDZblp08eRKBgYFQqVR1bqNWq6HVaq0eRERERI0OQQsWLEB4eDiWLVuGhIQErFq1CmlpaTh58iRSU1MxZ84cVFdX44477sCdd96J9PT0Bvc5ffp0LFmyBJ9++imOHTuGZ555BgaDwXK22JgxYzBr1ixL+2eeeQaFhYWYOnUqTp48idWrV2PevHmYPHlyM7pOREREUtboidG7d+/Gr7/+ih49etS5fsCAAXjiiSewePFiLFu2DNu2bUPnzp2vu8+kpCTk5+cjOTkZOTk5iI6Oxtq1ay2TpbOysiCX/5nTQkNDsW7dOjz//PPo3bs3goODMXXqVLz00kuN7QYRERERAEAmhBD2LqI16fV66HQ6FBcX2/zQWPjM1QAATxcn7E++w6b7JiIikrKW+P22ydlher0eq1atwrFjx2yxOyIiIqIW16wQNGrUKHzwwQcAgPLycsTGxmLUqFHo3bs3vvvuO5sWSERERNQSmhWCfv31VwwZMgQA8MMPP0AIgaKiIvzzn//E3/72N5sWSERERNQSmhWCiouL4eXlBQBYu3YtHnjgAbi4uGD48OGNOiuMiIiIyN6aFYJCQ0ORmpoKg8GAtWvX4o47rkwCvnz5MjQajU0LJCIiImoJzQpB06ZNw+jRoxESEoKgoCDccsstAK4cJuvVq5ct6yMiIqI2ojEnlK8/mouKqvZxY/Nm3UB10qRJiIuLQ1ZWFm6//XbLtXwiIyM5J4iIiKiVmM0Cy37LxPLfMpCnN8JYbUaIpzO0GifMGdEdcZHeAID9WZcR6esGtVIOuUwGsxB4a81xmIXAjMSu+GjraSzcfBoeLk4I8XSGyQx4uTqhvNIEpUKOy4ZKpOeVWl63X5gHlAo5zuSXorLaDG83NYI9nHHuchnOXipDr2AdVv5fPJxVivpKbxOafRf5mJgYxMTEWC0bPnz4DRdEREREjfPdvvN446ejVsvOXy4HUI75v5zEyonx+G7vebzwzQG4a5SoMpnhpJAjQKuxhJrPUs9ati0qq0JRWVWDr7svq8jqub6iGhkFBsvzvmEebT4AAU0IQW+99RamTp0KZ2fnBtvu3LkTBQUFDEVEREQt6Nu95y3/fmRAKIJ0zsjWV+CrnVnYlVmIlDXH8NHWMwCAkopqAEBFlRklFaV17u+hmBBUmwXkMhlUSjkCdRpoNUpsOpGPnWcuwVhtxoAIL/QO1qGi2oQLl8sRoHNGR19XKOQyXCq9cjPz54Zd/44RbUWjQ9DRo0cRFhaGhx56CCNGjEBsbCx8fX0BANXV1Th69Ci2b9+OL774AhcvXsRnn33WYkW3dZK6BDcREdnNJcOV0PHVU3G4qZOPZXlaVhGOZustAagu/x4biyc/3WN5/v7D0bgvOrjOtuMGRdio4ral0ROjP/vsM2zYsAFVVVV49NFHERAQAJVKBXd3d6jVavTt2xdLly7FmDFjcPz4cdx8880tWTcREZHkFZdfOXSldXayWj4gwuu626mUcgyL8kekjysA4PmELri3T1DLFNmGNWlOUJ8+fbBkyRJ89NFHOHjwIM6ePYvy8nL4+PggOjoaPj4+De+EiIiIbpgQwhKCdNeEoCcGReB/By4iIcofr4/sgTWHcvDjgYuYe28PfLPnHEb2vTLi89WEgbhkMKJHkK7V628LmjUxWi6XIzo6GtHR0TYuh4iIiBqjosqMymozAMDDxToEhXm7YO/s2y3PR/YNtgSf6Xd0tSwP0GkQoJPu9f1scgNVIiIial01o0AKuQxu6maf7C1pDEFERETtkGU+kEYJmUxm52raJ0ZHIiKidsRsFth6Mh8ncksAAB4uKjtX1H4xBBEREdnZkYvF+PHARUT6uKJnsA6n8w1wksuQeuYSMgoMUCsVcFLIsC/rMnL1RqtttRr+lDfXDb1zp06dwunTp3HzzTfD2dkZQggOyREREV0jPbcEJ3NLUVZZjfS8UqSevoRwH1dkF5XjTIEBhX9c76c5+oR62K5QiWlWCLp06RKSkpKwadMmyGQypKenIzIyEk8++SQ8PT0xf/58W9dJRETUpgghUFZpwuWySlwsqoC3mwoXi8qRU1yB3ZmFWHs4ByqlHAWldQecQxeKrZ4r5DKYzNaX2/V0cUL/cC8M7uyDymozOvm5oXuQFm5qJc5eKoNKKUeIZ8N3cqC6NSsEPf/881AqlcjKykJUVJRleVJSEqZPn84QREREDufwhWLM+/kYjmXrYTKL6wacxrizRwA6+rkiPtIHATo1In3cUFhWiVX7L+C+6GB4uDhBKZfVe4QlKlDb7NemK5oVgn755ResW7cOISEhVss7d+6Ms2fP1rMVERFR+1RcVoUxS3c1+rDV4E4+MFabcFNHH4R5uSCjwICyShO6Bbjjtig/eLqooJDXDjc+bmo8NSTS1uVTPZoVggwGA1xcXGotLywshFqtvuGiiIiI2pINx3KtAtC4m8IxtIsvOvq6Ib/UiDAvFzirFMgsMKC8yoT+4de/bQW1Dc0KQUOGDMFnn32GN954AwAgk8lgNpvx9ttv49Zbb7VpgURERPamr7hyTZ57egfig0f7Wa0L8/5zUKBnsDRvP9FeNSsEvf322xg2bBj27NmDyspKvPjiizhy5AgKCwuxY8cOW9dIRERkV2WVJgCAi0ph50rIlpp1xeiePXvi5MmTGDx4MO677z4YDAbcf//92L9/Pzp27GjrGomIiOyq/I8Q5OzEEORImn2dIJ1Oh1deecWWtRAREbVJ5VV/hCAVL0zoSJr9aVZUVODgwYPIy8uD2Wy2WnfvvffecGFERERtBQ+HOaZmhaC1a9dizJgxKCgoqLVOJpPBZDLdcGFERERtRXllNQAeDnM0zZoT9Oyzz+Khhx5CdnY2zGaz1YMBiIiIHM2fh8MYghxJs0JQbm4upk+fDn9/f1vXQ0RE1ObwcJhjalYIevDBB7FlyxYbl0JERNQ28ewwx9SsOUEffPABHnroIWzbtg29evWCk5OT1frnnnvOJsURERG1BTwc5piaFYL+85//4JdffoFGo8GWLVusbu4mk8kYgoiIyKFwJMgxNSsEvfLKK5g7dy5mzpwJubxZR9SIiIjajT/nBPE6QY6kWZ9mZWUlkpKSGICIiMhhCSFw+IIepcZq5OgrAPBwmKNpVggaO3YsVqxYgZdfftnW9RAREbUos1lg/7kiRPq44nR+KVRKOfZkXsbf1x6HsdoMZycFYsM9ceFyOc4UGCzbyWWAh4vTdfZM7U2zQpDJZMLbb7+NdevWoXfv3rUmRi9YsMAmxRERETWW2SxQbRbILzXCSS6DSinHsewS7MooRHZxOXZnFuJ0vqHB/ZRXmbAt/crFgFVKOQJ1GpRWVOPpmyPh46Zu6W5QK2pWCDp06BD69u0LADh8+LDVuqsnSRMREdlaXkkFjl7U4/CFYhSVVeGSoRK5+grsy7qMiipzwzu4hqtKAX+txjLqkxDlhzu6B0CllGNQJx/4ujP4OKpmhaDNmzfbug6HIoS9KyAiar+yi8vxn13nUFltRlahAXER3vh+/wU4O8lxKs+AglJjk/fZI0iLcG9XaJ2V6OjrhoPni7H2SA7+dl9PjOof2gK9oPaA09yJiKjNEEJg7NJdOJlbaln286GcettrnOS4v18IwrxcEOnjim4BWhSVV8LHTY3dmYXoEaRDB28XOClqn8hTWW2GSskTfKSs0SHo/vvvx/Lly6HVanH//fdft+33339/w4UREZH0lBqrLQEo1MsZl0oroZDLEBWoRf9wT1SbBYZ28cWXO7PQPVCLybd2qrWPMLgAAO6LDr7uazEAUaNDkE6ns8z30el0LVYQERFJV1FZFQBArZRj24u3AbgyOnTtfNObOvq0em3keBodgpYtW4bXX38dM2bMwLJly1qyJiIikqji8ishSOf851nHPOGGWkqTxgLnzp2L0tLShhsSERE1Q00I4vV4qDU0KQQJnvZEREQtqK6RIKKW0uRZYRyWJCKillIzJ0jnrLJzJSQFTT5FvkuXLg0GocLCwmYXRERE0lVUXgmAh8OodTQ5BM2dO5dnhxER0XWVGqvh7KSAQn7lf5qrTWbIZDIo5DIIIbAzoxA5xRXIKDBgx6kCBOg0OH+5HGnnigDwcBi1jiaHoIcffhh+fn4tUQsREbVhQghsOZEPrbMT1h3JgZNCBq3mz7AyMNIbpcZqHMvW42+rjwG4cguKAJ0G/9l1DiazgFajhL6iusHXCvJwbrF+ENVoUgjifCAiIsdTUWVCeaUJnq4qmMwCPx28iH1nL6PUaIKvuxrF5VXYcCwXRWWVqDI17QSZDcfyrJ5fG4BiO3jCX6tBsKczPF1U8HBxQrXJjPv7hdxwv4ga0qQQxLPDiIgcy9rD2Zj4xT4AgEohh0wGGKsbdxNSrUaJwZ19UFBSiV2Z9c8F1WqUGBjpjXOXy3FP70AM6eyDKpNAvzAP/s812VWTQpDZ3PS78xIRUdv07d7zmPHNAcvzStOVv/GuKgVu7eYHHzc1zhQYoPgjp/QL80S3QC283VQI83KBj9ufd1dfdyQH29Lz8UC/EHQL0EKpkEEpl8EsrvwPtLKOe3cR2RtvoEpEJFFbT+Zb/j37nu4Y0tkHcpkMgToNXNVN+3lI7BGAxB4BtZZfCVAc7aG2iSGIiEiiyitNAICU+3vhkQFhdq6GqPVxfJKISKIqqq6EIGcnhZ0rIbKPNhGCFi5ciPDwcGg0GsTFxWHXrl2N2u7rr7+GTCbDyJEjW7ZAIiIHVBOCNE5t4qeAqNXZ/Zu/YsUKTJ8+HXPmzMG+ffvQp08fJCYmIi8v77rbZWZmYsaMGRgyZEgrVUpE5FjKLSGII0EkTXYPQQsWLMCECRMwfvx4dO/eHYsXL4aLiwuWLl1a7zYmkwmjR4/G3LlzERkZ2YrVEhE5jnIeDiOJs2sIqqysxN69e5GQkGBZJpfLkZCQgNTU1Hq3e/311+Hn54cnn3yywdcwGo3Q6/VWDyIiAoxVV06J50gQSZVdQ1BBQQFMJhP8/f2tlvv7+yMnJ6fObbZv345///vfWLJkSaNeIyUlBTqdzvIIDQ294bqJiByBZSRIxRBE0mT3w2FNUVJSgscffxxLliyBj49Po7aZNWsWiouLLY9z5861cJVERO1DzSnyPBxGUmXX6wT5+PhAoVAgNzfXanlubi4CAmpfdOv06dPIzMzEiBEjLMtqrmKtVCpx4sQJdOzY0WobtVoNtVoNIiL6kxACFdVXQpCaZ4eRRNn1m69SqRATE4ONGzdalpnNZmzcuBHx8fG12nfr1g2HDh1CWlqa5XHvvffi1ltvRVpaGg91ERE1krHajJrbQXIkiKTK7leMnj59OsaOHYvY2FgMGDAA7733HgwGA8aPHw8AGDNmDIKDg5GSkgKNRoOePXtabe/h4QEAtZYTEVH9aq4RBHBiNEmX3UNQUlIS8vPzkZycjJycHERHR2Pt2rWWydJZWVmQyzlUS0RkSxV/nBmmlMvgxJubkkTZPQQBwJQpUzBlypQ6123ZsuW62y5fvtz2BTWDqBlXJiJqY0xmgf1Zl3GxuAI/H8yGTAY8GBMCgIfCSNraRAgiIiKgymRGaUU1PF1Vzd5Hnr4CDy/5HTpnJ1RUmXEsu+5ro605fOUyJBqeHk8SxhDUAjgqRETXEkIg+b9HcLawDC8mdkVnfzcs2nIaq/ZfgMZJgbJKE7IKy6CUyzAmPhxJ/UPRwdsFy3/LxLFsPSqqTPBwVqHKbIZCJkOOvgLVJgF/rRrpeaUwGKuReamsyXW5q/kzQNLFb7+NMPcQ0fUcvqDH57+fBQD8ejK/3nbVZoGlOzKwdEeGTV63o68rpt/eFVGB7oj0dUN6bgl+P3MJyT8egaeLCjPv6maT1yFqjxiCiIhaWGW1GSM+2F7v+km3dISx2gxXlQLnLpdjW3oBCg1GmMWVkZqk/qEI83ZB1qUy5Ogr4OmiQq8QHWQAMgoMkMtkOJqtx4XL5Rjc2Qd3dPeHANAvzBMqpfWk587+7ujs747hvYPgrlFyUjRJGkOQjXAgiIjqs+N0geXfN3fxRYinM84VlqGznzuSR3Svc5tTeaXILi7HwEjvFgkqXjcw74jIUTAEERG1sNziCsu/l43rD4Vc1uA2nfzc0MnPrSXLIpI8joPaCCdDE1F98kqMAICk2NBGBSAiah0MQURELSyv5MpIkL+W9zEkaksYgmyE40BEVJ9c/ZWRIF+txs6VENHVOCeIiMiGcvUV+G7feaw5dOVihN0C3LH+aC4AwN+dI0FEbQlDEBFRIwghkKOvgAwyFJVX4qudWcjVV2BAhDc+T82Em0aJ7KIKXDJUWm136EIxAEAuA6ICtfYonYjqwRBkI5wXTdS+lVeaUFBqxP5zRegTosOFonJ4OKuw4Vgutp8qwMncEhSVVdXabt2R3FrLOvu54WJROQyVJvQM1kKlkGPsTeEI9XJpja4QUSMxBBGRw9mTWYj1R3Mxok8QegbrLMuFEDh/uRzf7D2P7KJy+LirIQNw5KIeW69zFefrCfVyRnmlCYM7+eCBmBD4uqvR1d8dMhnPAiNq6xiCbERwajRRm/C3n47ik+1Xbjnx0a9n4K9VQ4Yr99pSK+UwVpubvM+4CC/c3MUX3QLcoVTIcXNnH8hkMhirTVAreQNSovaKIYiIHMp/D1y0el5zZhaAWgFoeK9AaJ2V6OLvjkJDJS4WVeDRuFAYjCb0D/eCXA7oy6vh46aqc2SHAYiofWMIshHOCSJqGyoqTQCAtdOG4Ey+AWqlHCqlHEcu6tE9UItQLxdonORQKxWNunWErzuDDpGjYggiIochhEBZ1ZUQ5Omiwt29/jwba0hnX3uVRURtFC+WSEQOo8okYDJfGZbVOHEEh4iujyGIiBxG+R+HwgDAmSGIiBrAEEREDqP8j0NhSrkMKiX/vBHR9fGvhI1wYjSR/dWEII4CEVFjMAQRkcMoq6wGAGhUDEFE1DCGIBvhxRKJ7K/ij5EgF4YgImoEhiAichjllVcuhsjDYUTUGAxBNsI5QUT2ZzkcxhBERI3AEEREDqOch8OIqAkYgmyEA0FE9lfBs8OIqAl42wwianeqTWZsOp6H/FIjOvm6QamQQSmX45UfDgPg2WFE1DgMQTYiOCmI6IZVm8yQyWQ4f7kMWo0TDJXV2JdVhJ8PZuPQhWIM6ewDpUKG9Udzre4Ofy2txqkVqyai9oohiIjsotBQCSeFDKfzDdh55hL2ZxVh7ZEcqzYKucxyLzAA+Hr3Ocu/3dVK9AzW4dCFYqiUchSXV0Hn7ISBkV54cnB4a3WDiNoxhqAWwDEhkqqrR0RP5JYgI9+ArSfzER3qgYxLBpzMKUGhoRIHzhc3an8ms0BnPze4a5QYEOGNC0XlMJnN6BvqiccGdoDzVYe9isur4KpSQKngVEciahyGIBth8CEpSs8twcs/HEKYlysUcmDjsTyUVFRDpZSj1FhtaXf1CE5DhnT2wei4DpDLAB93NfqFeTZqO50zD4ERUdMwBBFRs2xPL8Bj/94JANidedlqXaXJXOc2L9/dDc5OCry3IR1aZydMS+iMxB4BMJkFXNVKVFabeeNTImo1DEE2wnnRJDVbT+ZZPY8O9UCfEB0SewYgPbcUvu5qxHTwhM7ZCTszCjEw0gtq5ZXDV4/Hh9e5TwYgImpNDEFE1CyFhioAwF8TuyKpfyh83NSWdTd19LFqO7SLb6vWRkTUGPzfLlvhSBBJzOWySgCAj5vKKgAREbUXDEFE1CyFhishyNNFZedKiIiahyHIRgSHgkhiiv4YCfJyZQgiovaJIYiImsUyEsQQRETtFCdGE1EtpcZqfPpbJvqFeWJ3ZiHOXy6DDDKsP5YL4M8ABABePBxGRO0UQ5CN8BR5ak+EEDiVV4r8EiNCvVyQeuYSMgoMOJatx57MyzCZBcr/uCP79TgpZNDyIoVE1E4xBBE5kIwCA1zVCuTpjfj9zCVonBQoNFQi85IBVSYBd40SBSVG/HI0t0n77ROiQ+8QD8R39MblskpkXSqDSilHqJcLFHJZC/WGiKhlMQTZCAeCqDXlFFfg+/3noVYq0CtYh39tSse29IJm789fq8atXf3golLCVa1AuLcr7ujhj4tFFfBwcYK/VmPD6omI2gaGIKJ24FKpERuP5WH1oWxkFBiQVVjW4DberipE+rrC21WNXiE6yGRAnt4IjZMC7holjFUm3Nc3GO5qJbTOTtA4KWrto2sAD3URkeNiCLIRwUlB1EIMxmrc+s4W6CuqrZb3CtahosqE9LxSDAj3gkkI3NsnCN0C3OGqVqJnsM5OFRMRtQ8MQURtXHZxhSUA/TWxK2I6eKKLvzu8XFUwmwWKy6t4mjoRUTMwBNkIx4GopVT8cZZWgFaDybd2slonl8sYgIiImokXS2wJTERkQzUhyFlVe84OERE1H0MQURtXc70etZL/uRIR2RL/qtrI1fOiORBEtlRRZQaAOs/eIiKi5mMIImrjLIfDGIKIiGyKIchGrr6LPE+XJ1uqORymceJ/rkREtsS/qkRtnJETo4mIWgRDkK1wThC1EMtIkJIhiIjIltpECFq4cCHCw8Oh0WgQFxeHXbt21dt2yZIlGDJkCDw9PeHp6YmEhITrtidq72omRqs5J4iIyKbsHoJWrFiB6dOnY86cOdi3bx/69OmDxMRE5OXl1dl+y5YteOSRR7B582akpqYiNDQUd9xxBy5cuNDKlVu7evSHU4LIlso5MZqIqEXYPQQtWLAAEyZMwPjx49G9e3csXrwYLi4uWLp0aZ3tv/zyS0yaNAnR0dHo1q0bPvnkE5jNZmzcuLGVKydqHRWcGE1E1CLs+le1srISe/fuRUJCgmWZXC5HQkICUlNTG7WPsrIyVFVVwcvLq6XKbBTr6wRxKIhsh6fIExG1DLveO6ygoAAmkwn+/v5Wy/39/XH8+PFG7eOll15CUFCQVZC6mtFohNFotDzX6/XNL5iolQkhUFZZMxLEEEREZEvt+gaqb731Fr7++mts2bIFGo2mzjYpKSmYO3duq9bFOUHUGEIIyGQyFJVVwkkhh6taiVx9BTYfz8Phi8U4fEGPc4VluGSoBMDDYUREtmbXEOTj4wOFQoHc3Fyr5bm5uQgICLjutu+88w7eeustbNiwAb1796633axZszB9+nTLc71ej9DQ0BsrvA48BEYNKTVWI7uoHFUmgW/2nsOPaRdRWW1GibEaKoUc/jo1zhWW17s9R4KIiGzLriFIpVIhJiYGGzduxMiRIwHAMsl5ypQp9W739ttv480338S6desQGxt73ddQq9VQq9W2LLtBjEPSdPSiHmYhEObtgmqTwMWicmw/VYDi8iq4a5T49LdM5OqNdW5baTJbApCnixNu6uiDglIjogK1UDvJsT+rCDd38W3N7hAROTy7Hw6bPn06xo4di9jYWAwYMADvvfceDAYDxo8fDwAYM2YMgoODkZKSAgD4+9//juTkZHz11VcIDw9HTk4OAMDNzQ1ubm526wcPgTmWKpMZJrNAoaESuzML0S1Ai92ZhUg9fQlaZyf4uauRdq4I+SVG+LqrUVRWiQPnixu9f42THE8OjoBaqUBnPzeEerlAX1EFD2cVugW4Qy6XtWDviIgIaAMhKCkpCfn5+UhOTkZOTg6io6Oxdu1ay2TprKwsyOV/zoVYtGgRKisr8eCDD1rtZ86cOXjttddas/T6MRC1W8t3ZGDhltPQl1fBWG1u3EbZDTe5o7s/BkZ6Y3BnHyjlMrhplPBzr3seGxERtQ67hyAAmDJlSr2Hv7Zs2WL1PDMzs+ULagbmHsfw/f4LyC+pfchKLgP6hXkiNtwLpcYqXDZUQamQoWuAO6pNAgZjNaJDPdDB2xUZBQYE6DQI8XRGRZUJHbxd7dATIiJqSJsIQY6Gk6Tbr8I/zsR6dXgUQjxd0DNYi2APZwCATNa4Q1Tdg7QtVh8REdkOQ5CNCE4KcgiX/whBCVH+CPfhCA4RkSPjhUdaAPNQ+2SsNsHwx4UJPV1Vdq6GiIhaGkMQ0R+KyqoAAAq5DFoNB0mJiBwdQ5CNcPSn/auZD+Tp4tTo+T9ERNR+8X93WwDzUNtXUWVCQakRl0orUWi48vj39gwAgM7Zyc7VERFRa2AIIruquX/W9VSbzFi55zwCdGoUGqqw6XguMgvKMLx3IJL6h8LZSYHdmYVwVSuxP+syCg1VWH80B6fzDfBxU8PHTYXLZZXQl1fDw8UJ+vIqy9yfuvi6t+4VxomIyD4YgloAzxRrnDWHspH84xFEeLtieO9A/Ha6AKfzDQj1dIax2oyTuaWoNptRUlENk7n2e3o0W4/FW07DVa1Ejr6iztcoKDWioPTP6/6UF/8ZflQKOXzcVPByU8FFpURXf3cUlVfhwZgQ23eWiIjaHIYgG2HuaZpzhWWYtiINxmoz8kuM2JVZaFl3Kq+03u0ifV1RZTLDXe2EorJKXCyuQImxula7JwZFQOMkx6XSSvhp1YjwcUXPYB3KKk3QOTvB200Fd7WSc3+IiCSMIagFMA81bMepAhirzdA5O2FIZx8UGirRwdsVzk4KKBUymM0CPYK1CPNygaeLCkEezjALARfVn19ZY7UJqw9mQyYDtBonmMwCt3f3R1FZFU9xJyKiBjEE2QivEt00pX+M3tza1RfvPdy3WftQKxW4v1/tQ1cMQERE1Bg8Rb4F8NBYw8r+mJjsomYOJyIi+2AIIrswVF4ZCXJVKexcCRERSRVDkI1w9Kdpyox/jASpOBJERET2wRBEdmEZCVJzJIiIiOyDIchGrh0I4rWCro8jQUREZG8MQWQXNSNBLpwTREREdsIQZCPXjvxwIOj6LGeHcSSIiIjshCGI7MJg5JwgIiKyL4YgG6k1J8guVbQfHAkiIiJ74y8Q2YzZLHA8pwTdAtwhl8tqrZv05T6knrmErv7uyCosA8CRICIish+GoBZyZY6Q496cs7LaDH1FFXzc1BBCoNosMP+Xk1i89TTu6R2I0XEd8NvpAvw37SJMZgF3jRLHc0oAwOpmqa4cCSIiIjvhL5CNOMpE6CqTGZuP5+FiUTkifd0Q7u2KHacLsD29AB28XTCksy/iO3pjwmd7sPVkPjxcnFBeaYKx2mzZx08Hs/HTwexa+1bIZbi1qy9cVEpcMhgRpHNGiKdza3aPiIjIgiGohbTFTFRUVglXtRJOCjmKyiphFsDes5dRaqzCkM6+yCosw+v/O4q0c0X17uPDLafx2oju2Hoy/499Vlmt1zk7wWwWMAmBhCh/DO7kA5VSDpNZoF8HT0T4uLZkF4mIiBqNIchm2mLsueK3UwVYtPU0tp8qgAxAbAcvq0NSDXFVKfBQbChO55diW3oBXvvfUcu6L56Mg8ZJjhBPF6iUcni5qlBZbYZcBigVnHdPRERtF0NQC2krh8dMZoEnP92D8qorZ2MJoN4A5OHihJgwTzw3rDN6BGlxoagcQR7OkOFKoDGZBWL/th6X/xj9ua2bHwZ39qm1H5WS4YeIiNo+hiAbaSuh51qGympLAFo2rj8OXSiGWimHp6sKvYJ10DgpYDBWI9jDGR4uTpDJ/pzM3cHb+tCVQi7DuJsi8O6GkxgQ7oXpt3dp1b4QERHZEkNQCxFt5PBYacWVixI6KWS4tZsfbu3md0P7e/a2TkjqH4oAncYW5REREdkNj1vYSNuIPLXVXJnZTW2bvCuXyxiAiIjIITAEtZC2cnisxHJ7Cg76ERERXY0hyEbaSui5lq1HgoiIiBwFQ5CDq5kTxBBERERkjSGoDfhgUzq+3pXVIvsurRkJ0jAEERERXY2/jDbS3LPBTuWV4J1fTgIAHowJsfkFBks5J4iIiKhOHAlqIfqKqoYbASg0/NmuoLSyya/z68l8zP/lBIzVpjrX18wJcmcIIiIissJfRhu5dmL0hcvl8HNv+FTyS6VGy79z9BUNnn7+n11ZOFdYBpkMiPRxwwvfHAAAfLItA+881Af9IzxxrrAMh84X43JZFd7fmA6AI0FERETX4i9jCzl3uRx9wzwbbJd/dQgqLoc5WIezhWVwVSmQnleKuAgvLP8tE9vSCxDi6Ywvd9Y9d6i8yoTJX+2r93XcOSeIiIjICn8ZbeTakaDzl8vqbfvhllP47LeziOngiSCPP0d+Mi+VYca3B/D9vgs2qytQp0GvYB3uiw622T6JiIgcAUNQC8nIN8BgrMb5y+WI9HVFdlEFdM5OOHC+CG+vPQEAWH0o22qbt9Ycv+4+B3fygZerCvklRhzP0SPS1w1J/UNxX3QQ/ncgGzIAAzt6I0CrgUIuQ0aBASGeznDi3dyJiIhqYQiykWvPDvtm73l8s/f8De1z0i0dMXpgB2g1SrhrnK7b9sGYkFrLInxc62hJREREAEOQzXm6OKHaJCy3q6hLsIczVk6Mx+6MQuicnZB5yYC4CG/k6MvR0dfNcpp8sIdza5VNREQkOQxBNqZSypFyfw/szyrCo3Fh8HRVoaLKBBeVEpkFBlSZzOga4A4XlRLBfa3n6XQP0tqpaiIiIulhCLKRqydG39kzEHf2DLQ81/5xKKtnsK61yyIiIqJ6cMasjckgs3cJRERE1AgMQURERCRJDEE2JuNAEBERUbvAEGQj114skYiIiNo2hiAb40AQERFR+8AQZCPXXiyRiIiI2jaGIBuTcVIQERFRu8AQRERERJLEEGQjnBhNRETUvjAEERERkSQxBNkIB4KIiIjaF4YgG+O8aCIiovahTYSghQsXIjw8HBqNBnFxcdi1a9d123/zzTfo1q0bNBoNevXqhZ9//rmVKq2f4KQgIiKidsXuIWjFihWYPn065syZg3379qFPnz5ITExEXl5ene1/++03PPLII3jyySexf/9+jBw5EiNHjsThw4dbufK6cSSIiIiofZAJOw9hxMXFoX///vjggw8AAGazGaGhoXj22Wcxc+bMWu2TkpJgMBjw008/WZYNHDgQ0dHRWLx4cYOvp9frodPpUFxcDK1Wa7N+7Mu6jPs//A2hXs7Y9uJtNtsvERERtczvt11HgiorK7F3714kJCRYlsnlciQkJCA1NbXObVJTU63aA0BiYmK97Y1GI/R6vdWjJcl44wwiIqJ2wa4hqKCgACaTCf7+/lbL/f39kZOTU+c2OTk5TWqfkpICnU5neYSGhtqm+GvIZTJonORQK+1+hJGIiIgaweF/sWfNmoXi4mLL49y5cy3yOtGhHjj+xl1YP31oi+yfiIiIbEtpzxf38fGBQqFAbm6u1fLc3FwEBATUuU1AQECT2qvVaqjVatsUTERERA7DriNBKpUKMTEx2Lhxo2WZ2WzGxo0bER8fX+c28fHxVu0BYP369fW2JyIiIqqLXUeCAGD69OkYO3YsYmNjMWDAALz33nswGAwYP348AGDMmDEIDg5GSkoKAGDq1KkYOnQo5s+fj+HDh+Prr7/Gnj178PHHH9uzG0RERNTO2D0EJSUlIT8/H8nJycjJyUF0dDTWrl1rmfyclZUFufzPAaubbroJX331FV599VW8/PLL6Ny5M1atWoWePXvaqwtERETUDtn9OkGtraWuE0REREQtx+GuE0RERERkLwxBREREJEkMQURERCRJDEFEREQkSQxBREREJEkMQURERCRJDEFEREQkSQxBREREJEkMQURERCRJdr9tRmuruUC2Xq+3cyVERETUWDW/27a80YXkQlBJSQkAIDQ01M6VEBERUVOVlJRAp9PZZF+Su3eY2WzGxYsX4e7uDplMZtN96/V6hIaG4ty5cw5/XzKp9FUq/QTYV0fFvjoeqfQTsO6ru7s7SkpKEBQUZHVj9RshuZEguVyOkJCQFn0NrVbr8F/MGlLpq1T6CbCvjop9dTxS6SfwZ19tNQJUgxOjiYiISJIYgoiIiEiSGIJsSK1WY86cOVCr1fYupcVJpa9S6SfAvjoq9tXxSKWfQMv3VXITo4mIiIgAjgQRERGRRDEEERERkSQxBBEREZEkMQQRERGRJDEE2cjChQsRHh4OjUaDuLg47Nq1y94lNdmvv/6KESNGICgoCDKZDKtWrbJaL4RAcnIyAgMD4ezsjISEBKSnp1u1KSwsxOjRo6HVauHh4YEnn3wSpaWlrdiLhqWkpKB///5wd3eHn58fRo4ciRMnTli1qaiowOTJk+Ht7Q03Nzc88MADyM3NtWqTlZWF4cOHw8XFBX5+fvjrX/+K6urq1uxKgxYtWoTevXtbLjQWHx+PNWvWWNY7Sj/r8tZbb0Emk2HatGmWZY7S39deew0ymczq0a1bN8t6R+knAFy4cAGPPfYYvL294ezsjF69emHPnj2W9Y7ydyk8PLzWZyqTyTB58mQAjvWZmkwmzJ49GxEREXB2dkbHjh3xxhtvWN0TrNU+V0E37OuvvxYqlUosXbpUHDlyREyYMEF4eHiI3Nxce5fWJD///LN45ZVXxPfffy8AiB9++MFq/VtvvSV0Op1YtWqVOHDggLj33ntFRESEKC8vt7S58847RZ8+fcTvv/8utm3bJjp16iQeeeSRVu7J9SUmJoply5aJw4cPi7S0NHH33XeLsLAwUVpaamkzceJEERoaKjZu3Cj27NkjBg4cKG666SbL+urqatGzZ0+RkJAg9u/fL37++Wfh4+MjZs2aZY8u1evHH38Uq1evFidPnhQnTpwQL7/8snBychKHDx8WQjhOP6+1a9cuER4eLnr37i2mTp1qWe4o/Z0zZ47o0aOHyM7Otjzy8/Mt6x2ln4WFhaJDhw5i3LhxYufOneLMmTNi3bp14tSpU5Y2jvJ3KS8vz+rzXL9+vQAgNm/eLIRwnM9UCCHefPNN4e3tLX766SeRkZEhvvnmG+Hm5ibef/99S5vW+lwZgmxgwIABYvLkyZbnJpNJBAUFiZSUFDtWdWOuDUFms1kEBASIf/zjH5ZlRUVFQq1Wi//85z9CCCGOHj0qAIjdu3db2qxZs0bIZDJx4cKFVqu9qfLy8gQAsXXrViHElX45OTmJb775xtLm2LFjAoBITU0VQlwJjHK5XOTk5FjaLFq0SGi1WmE0Glu3A03k6ekpPvnkE4ftZ0lJiejcubNYv369GDp0qCUEOVJ/58yZI/r06VPnOkfq50svvSQGDx5c73pH/rs0depU0bFjR2E2mx3qMxVCiOHDh4snnnjCatn9998vRo8eLYRo3c+Vh8NuUGVlJfbu3YuEhATLMrlcjoSEBKSmptqxMtvKyMhATk6OVT91Oh3i4uIs/UxNTYWHhwdiY2MtbRISEiCXy7Fz585Wr7mxiouLAQBeXl4AgL1796Kqqsqqr926dUNYWJhVX3v16gV/f39Lm8TEROj1ehw5cqQVq288k8mEr7/+GgaDAfHx8Q7bz8mTJ2P48OFW/QIc73NNT09HUFAQIiMjMXr0aGRlZQFwrH7++OOPiI2NxUMPPQQ/Pz/07dsXS5Yssax31L9LlZWV+OKLL/DEE09AJpM51GcKADfddBM2btyIkydPAgAOHDiA7du346677gLQup+r5G6gamsFBQUwmUxWXzwA8Pf3x/Hjx+1Ule3l5OQAQJ39rFmXk5MDPz8/q/VKpRJeXl6WNm2N2WzGtGnTMGjQIPTs2RPAlX6oVCp4eHhYtb22r3W9FzXr2pJDhw4hPj4eFRUVcHNzww8//IDu3bsjLS3NofoJAF9//TX27duH3bt311rnSJ9rXFwcli9fjq5duyI7Oxtz587FkCFDcPjwYYfq55kzZ7Bo0SJMnz4dL7/8Mnbv3o3nnnsOKpUKY8eOddi/S6tWrUJRURHGjRsHwLG+uwAwc+ZM6PV6dOvWDQqFAiaTCW+++SZGjx4NoHV/bxiCSNImT56Mw4cPY/v27fYupcV07doVaWlpKC4uxrfffouxY8di69at9i7L5s6dO4epU6di/fr10Gg09i6nRdX8HzMA9O7dG3FxcejQoQNWrlwJZ2dnO1ZmW2azGbGxsZg3bx4AoG/fvjh8+DAWL16MsWPH2rm6lvPvf/8bd911F4KCguxdSotYuXIlvvzyS3z11Vfo0aMH0tLSMG3aNAQFBbX658rDYTfIx8cHCoWi1iz93NxcBAQE2Kkq26vpy/X6GRAQgLy8PKv11dXVKCwsbJPvxZQpU/DTTz9h8+bNCAkJsSwPCAhAZWUlioqKrNpf29e63ouadW2JSqVCp06dEBMTg5SUFPTp0wfvv/++w/Vz7969yMvLQ79+/aBUKqFUKrF161b885//hFKphL+/v0P192oeHh7o0qULTp065VCfa2BgILp37261LCoqynLozxH/Lp09exYbNmzAU089ZVnmSJ8pAPz1r3/FzJkz8fDDD6NXr154/PHH8fzzzyMlJQVA636uDEE3SKVSISYmBhs3brQsM5vN2LhxI+Lj4+1YmW1FREQgICDAqp96vR47d+609DM+Ph5FRUXYu3evpc2mTZtgNpsRFxfX6jXXRwiBKVOm4IcffsCmTZsQERFhtT4mJgZOTk5WfT1x4gSysrKs+nro0CGr/wjXr18PrVZb6492W2M2m2E0Gh2un8OGDcOhQ4eQlpZmecTGxmL06NGWfztSf69WWlqK06dPIzAw0KE+10GDBtW6fMXJkyfRoUMHAI71d6nGsmXL4Ofnh+HDh1uWOdJnCgBlZWWQy63jh0KhgNlsBtDKn+sNTPCmP3z99ddCrVaL5cuXi6NHj4qnn35aeHh4WM3Sbw9KSkrE/v37xf79+wUAsWDBArF//35x9uxZIcSVUxY9PDzEf//7X3Hw4EFx33331XnKYt++fcXOnTvF9u3bRefOndvcqajPPPOM0Ol0YsuWLVanpJaVlVnaTJw4UYSFhYlNmzaJPXv2iPj4eBEfH29ZX3M66h133CHS0tLE2rVrha+vb5s7HXXmzJli69atIiMjQxw8eFDMnDlTyGQy8csvvwghHKef9bn67DAhHKe/L7zwgtiyZYvIyMgQO3bsEAkJCcLHx0fk5eUJIRynn7t27RJKpVK8+eabIj09XXz55ZfCxcVFfPHFF5Y2jvJ3SYgrZxaHhYWJl156qdY6R/lMhRBi7NixIjg42HKK/Pfffy98fHzEiy++aGnTWp8rQ5CN/Otf/xJhYWFCpVKJAQMGiN9//93eJTXZ5s2bBYBaj7Fjxwohrpy2OHv2bOHv7y/UarUYNmyYOHHihNU+Ll26JB555BHh5uYmtFqtGD9+vCgpKbFDb+pXVx8BiGXLllnalJeXi0mTJglPT0/h4uIi/vKXv4js7Gyr/WRmZoq77rpLODs7Cx8fH/HCCy+IqqqqVu7N9T3xxBOiQ4cOQqVSCV9fXzFs2DBLABLCcfpZn2tDkKP0NykpSQQGBgqVSiWCg4NFUlKS1bVzHKWfQgjxv//9T/Ts2VOo1WrRrVs38fHHH1utd5S/S0IIsW7dOgGgVv1CONZnqtfrxdSpU0VYWJjQaDQiMjJSvPLKK1an8rfW5yoT4qpLNBIRERFJBOcEERERkSQxBBEREZEkMQQRERGRJDEEERERkSQxBBEREZEkMQQRERGRJDEEERERkSQxBBGRzWVmZkImkyEtLc3epbQZt9xyC6ZNm2bvMojoKgxBRFQnmUx23cdrr71m7xJraQtBY8uWLZDJZLVudklEbY/S3gUQUduUnZ1t+feKFSuQnJxsdTNLNzc3e5RFRGQzHAkiojoFBARYHjqdDjKZzPLcz88PCxYsQEhICNRqNaKjo7F27dp692UymfDEE0+gW7duyMrKAgD897//Rb9+/aDRaBAZGYm5c+eiurraso1MJsMnn3yCv/zlL3BxcUHnzp3x448/3lCftm/fjiFDhsDZ2RmhoaF47rnnYDAYLOvDw8Mxb948PPHEE3B3d0dYWBg+/vhjq3389ttviI6OhkajQWxsLFatWmU59JeZmYlbb70VAODp6QmZTIZx48ZZtjWbzXjxxRfh5eWFgICANjmaRiQlDEFE1GTvv/8+5s+fj3feeQcHDx5EYmIi7r33XqSnp9dqazQa8dBDDyEtLQ3btm1DWFgYtm3bhjFjxmDq1Kk4evQoPvroIyxfvhxvvvmm1bZz587FqFGjcPDgQdx9990YPXo0CgsLm1Xz6dOnceedd+KBBx7AwYMHsWLFCmzfvh1Tpkyxajd//nzExsZi//79mDRpEp555hnLCJher8eIESPQq1cv7Nu3D2+88QZeeukly7ahoaH47rvvAAAnTpxAdnY23n//fcv6Tz/9FK6urti5cyfefvttvP7661i/fn2z+kNENmCDG8ISkYNbtmyZ0Ol0ludBQUHizTfftGrTv39/MWnSJCGEEBkZGQKA2LZtmxg2bJgYPHiwKCoqsrQdNmyYmDdvntX2n3/+uQgMDLQ8ByBeffVVy/PS0lIBQKxZs6beOq+9a/zVnnzySfH0009bLdu2bZuQy+WivLxcCCFEhw4dxGOPPWZZbzabhZ+fn1i0aJEQQohFixYJb29vS3shhFiyZIkAIPbv3y+EEGLz5s0CgLh8+XKt2gYPHmy1rH///uKll16qtz9E1LI4J4iImkSv1+PixYsYNGiQ1fJBgwbhwIEDVsseeeQRhISEYNOmTXB2drYsP3DgAHbs2GE18mMymVBRUYGysjK4uLgAAHr37m1Z7+rqCq1Wi7y8vGbVfeDAARw8eBBffvmlZZkQAmazGRkZGYiKiqr1mjWHAGte88SJE+jduzc0Go2lzYABAxpdw9X7BoDAwMBm94eIbhxDEBG1mLvvvhtffPEFUlNTcdttt1mWl5aWYu7cubj//vtrbXN1wHBycrJaJ5PJYDabm1VLaWkp/u///g/PPfdcrXVhYWEt8prXasl9E1HTMQQRUZNotVoEBQVhx44dGDp0qGX5jh07ao2KPPPMM+jZsyfuvfderF692tK+X79+OHHiBDp16tRqdffr1w9Hjx69odfs2rUrvvjiCxiNRqjVagDA7t27rdqoVCoAV0a2iKhtYwgioib761//ijlz5qBjx46Ijo7GsmXLkJaWZnWoqcazzz4Lk8mEe+65B2vWrMHgwYORnJyMe+65B2FhYXjwwQchl8tx4MABHD58GH/7299uqLb8/PxaF2kMDAzESy+9hIEDB2LKlCl46qmn4OrqiqNHj2L9+vX44IMPGrXvRx99FK+88gqefvppzJw5E1lZWXjnnXcAXBnVAYAOHTpAJpPhp59+wt133w1nZ2deToCojeLZYUTUZM899xymT5+OF154Ab169cLatWvx448/onPnznW2nzZtGubOnYu7774bv/32GxITE/HTTz/hl19+Qf/+/TFw4EC8++676NChww3X9tVXX6Fv375WjyVLlqB3797YunUrTp48iSFDhqBv375ITk5GUFBQo/et1Wrxv//9D2lpaYiOjsYrr7yC5ORkAH8exgsODsbcuXMxc+ZM+Pv71zr7jIjaDpkQQti7CCKi9urLL7/E+PHjUVxcbDX5m4jaPh4OIyJqgs8++wyRkZEIDg7GgQMH8NJLL2HUqFEMQETtEEMQEVET5OTkIDk5GTk5OQgMDMRDDz1U6yKPRNQ+8HAYERERSRInRhMREZEkMQQRERGRJDEEERERkSQxBBEREZEkMQQRERGRJDEEERERkSQxBBEREZEkMQQRERGRJDEEERERkST9P1OeSZQTN1xEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2351741790771484"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows = 158866\n",
    "rows = 30000\n",
    "# cols = 32000\n",
    "cols = 10000\n",
    "total_floats = rows * cols\n",
    "size_of_float = 8  # bytes\n",
    "memory_cost_bytes = total_floats * size_of_float\n",
    "memory_cost_gb = memory_cost_bytes / (1024 ** 3)\n",
    "memory_cost_gb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

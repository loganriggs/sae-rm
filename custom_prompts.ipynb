{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Model & SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/new/sae-rm/rm_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/root/new/sae-rm/rm_env/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.09s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "torch.jit.is_tracing = lambda : True\n",
    "tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"reciprocate/dahoas-gptj-rm-static\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/new/sae-rm/rm_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.87s/it]\n"
     ]
    }
   ],
   "source": [
    "model_nnsight = LanguageModel(\n",
    "    model_name,\n",
    "    device_map = \"cpu\",\n",
    "    automodel = AutoModelForSequenceClassification,\n",
    "    dispatch = True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from dictionary import GatedAutoEncoder\n",
    "\n",
    "# Possible layers: 2,8,12,14,16,20\n",
    "layer = 12\n",
    "activation_name = f\"transformer.h.{layer}\"\n",
    "model_id = \"Elriggs/rm\"\n",
    "sae_file_save_name = f\"ae_layer{layer}\"\n",
    "sae_filename = sae_file_save_name + \".pt\"\n",
    "sae_file_dir = f\"sae_results/{sae_file_save_name}\"\n",
    "ae_download_location = hf_hub_download(repo_id=model_id, filename=sae_filename)\n",
    "sae = GatedAutoEncoder.from_pretrained(ae_download_location).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution Patching & HTML display definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from interp_utils import patching_effect_two\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from interp_utils import tokens_and_activations_to_html\n",
    "from IPython.display import HTML, display\n",
    "from einops import rearrange\n",
    "from baukit import Trace\n",
    "from interp_utils import get_autoencoder_activation\n",
    "from functools import partial\n",
    "\n",
    "def sae_ablation_after_pos(x, sae, feature_ind, positions):\n",
    "    # baukit nonsense to handle both residual stream & mlp/attn_output\n",
    "    if(isinstance(x, tuple)):\n",
    "        second_value = x[1]\n",
    "        internal_activation = x[0]\n",
    "    else:\n",
    "        internal_activation = x\n",
    "    batch, seq_len, hidden_size = internal_activation.shape\n",
    "    int_val = rearrange(internal_activation, \"b seq d_model -> (b seq) d_model\")\n",
    "    \n",
    "    # Encode in features, then remove all features\n",
    "    f = sae.encode(int_val)\n",
    "\n",
    "    residual = int_val - sae.decode(f)\n",
    "\n",
    "    # Ablate all fe\n",
    "    reshaped_f = rearrange(f, \"(b s) h -> b s h\", b=batch, s=seq_len)\n",
    "    for pos_ind, pos in enumerate(positions):\n",
    "        # reshaped_f[pos_ind, pos:, feature_ind] = 0\n",
    "        reshaped_f[pos_ind, pos:, feature_ind] = 0\n",
    "    ablated_f = rearrange(reshaped_f, \"b s h -> (b s) h\")\n",
    "\n",
    "    # Decode & add back in residual\n",
    "    x_hat = sae.decode(ablated_f)\n",
    "\n",
    "    x_recon = residual + x_hat\n",
    "\n",
    "\n",
    "    # baukit nonsense to handle both residual stream & mlp/attn_output\n",
    "    reconstruction = rearrange(x_recon, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "    if(isinstance(x, tuple)):\n",
    "        return_value = (reconstruction, second_value)\n",
    "    else:\n",
    "        return_value = reconstruction\n",
    "    return return_value\n",
    "\n",
    "def get_padding_indices(token_tensor, padding_token_id):\n",
    "\n",
    "    padding_indices = -torch.ones(token_tensor.size(0), dtype=torch.long)\n",
    "\n",
    "    # Iterate over each entry to find the first occurrence of the padding token\n",
    "    for i in range(token_tensor.size(0)):\n",
    "        padding_idx = (token_tensor[i] == padding_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(padding_idx) > 0:\n",
    "            padding_indices[i] = padding_idx[0]\n",
    "\n",
    "    return padding_indices\n",
    "\n",
    "def display_feature_activation_and_ablation(tokens, prefix_text, feature, feature_ablation, model, sae, activation_name, tokenizer):\n",
    "\n",
    "    prefix_tokens = tokenizer(prefix_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    prefix_size = len(prefix_tokens[0])\n",
    "\n",
    "    padding_location = get_padding_indices(tokens, tokenizer.pad_token_id)\n",
    "    # completion_tokens = tokenizer(custom_text_suffix, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "    # get the reward for each\n",
    "    batch_size, seq_size = tokens.shape\n",
    "    with torch.no_grad():\n",
    "        # Get the feature activations & reward\n",
    "        feature_activations, reward = get_autoencoder_activation(model, activation_name, tokens, sae, return_output=True)\n",
    "        feature_activations = feature_activations[..., feature].cpu()\n",
    "        reward = reward.squeeze().cpu()\n",
    "        feature_activations = rearrange(feature_activations, \"(b s) -> b s\", b=batch_size, s=seq_size)\n",
    "\n",
    "        #Get ablation reward\n",
    "        hook_function = partial(sae_ablation_after_pos, sae = sae, feature_ind=feature_ablation, positions=[prefix_size for _ in range(len(tokens))])\n",
    "        with Trace(model, activation_name, edit_output=hook_function) as _:\n",
    "            ablated_reward = model(tokens.to(model.device)).logits.cpu().squeeze()\n",
    "        \n",
    "    token_list = []\n",
    "    activation_list = []\n",
    "    text_above = []\n",
    "    length_of_prompt = prefix_size\n",
    "    prompt_tok = prefix_tokens.tolist()\n",
    "    prompt_act = feature_activations[0][:length_of_prompt].tolist()\n",
    "    token_list.append(prompt_tok)\n",
    "    activation_list.append(prompt_act)\n",
    "    text_above.append(f\"Prompt<br>\")\n",
    "\n",
    "    # for token_ind in range(len(completion_tokens)):\n",
    "    for token_ind in range(len(tokens)):\n",
    "        tok = tokens[token_ind]\n",
    "        act = feature_activations[token_ind]\n",
    "\n",
    "        pad_ind = padding_location[token_ind]\n",
    "        if pad_ind == -1:\n",
    "            pad_ind = len(tok)\n",
    "        # chosen_tok = tokens[token_ind][length_of_prompt:pad_ind].tolist()\n",
    "        suffix_tok = tok[length_of_prompt:pad_ind].tolist()\n",
    "        suffix_act = act[length_of_prompt:pad_ind].tolist()\n",
    "        suffix_reward = reward[token_ind].item()\n",
    "        ablated_suffix_reward = ablated_reward[token_ind].item()\n",
    "\n",
    "        # append\n",
    "        token_list.append(suffix_tok)\n",
    "        activation_list.append(suffix_act)\n",
    "        text_above.append(f\"Reward: {suffix_reward:.2f} -> {ablated_suffix_reward:.2f} <br> {token_ind+1}.\")\n",
    "        \n",
    "    html = tokens_and_activations_to_html(token_list, activation_list, tokenizer, logit_diffs=None, text_above_each_act=text_above)\n",
    "    print(f\"feature: {feature}\")\n",
    "    display(HTML(html))\n",
    "\n",
    "def attribution_patching(model, model_nnsight, sae, tokens, activation_name, prefix_size, tracer_kwargs, steps=10):\n",
    "    model.to(\"cpu\") \n",
    "    gc.collect()  \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    device = sae.decoder.weight.device\n",
    "    def get_reward(model):\n",
    "        return model.output.logits[:, 0]\n",
    "\n",
    "    model_nnsight.to(device)\n",
    "    # Get module by it's name\n",
    "    attributes = activation_name.split('.')\n",
    "    module = model_nnsight\n",
    "    for attr in attributes:\n",
    "        module = getattr(module, attr)\n",
    "\n",
    "    dictionaries = {}\n",
    "    submodule_names = {}\n",
    "    submodule_names[module] = activation_name\n",
    "    dictionaries[module] = sae\n",
    "    submodules = [module]\n",
    "    list_effects = []\n",
    "    for token in tokens:\n",
    "        pos = [prefix_size]\n",
    "        effects = patching_effect_two(\n",
    "            token.to(device),\n",
    "            None,\n",
    "            model_nnsight,\n",
    "            submodules = submodules,\n",
    "            dictionaries = dictionaries,\n",
    "            tracer_kwargs=tracer_kwargs,\n",
    "            positions = pos,\n",
    "            metric_fn = get_reward,\n",
    "            steps = steps,\n",
    "        )\n",
    "        list_effects.append(effects)\n",
    "    list_effects = torch.cat(list_effects)\n",
    "    model_nnsight.to(\"cpu\")\n",
    "    model.to(device)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return list_effects\n",
    "\n",
    "def ignore_baseline_features(list_effects, num_baselines, top_features_to_ignore=10):\n",
    "    num_baselines *= -1\n",
    "    ignore_these_features = list_effects[num_baselines:].abs().sum(0).sum(0).topk(top_features_to_ignore).indices\n",
    "    tmp_effects = list_effects.clone()\n",
    "    tmp_effects[:,:, ignore_these_features] = 0\n",
    "    return tmp_effects\n",
    "\n",
    "def true_ablation_effect(features_to_ablate, tokens, prefix_size):\n",
    "    all_reward_diffs = []\n",
    "    with torch.no_grad():\n",
    "        original_reward = model(tokens.to(device)).logits.cpu().squeeze()\n",
    "        for feat_to_ablate in tqdm(features_to_ablate):\n",
    "            #Get ablation reward\n",
    "            hook_function = partial(sae_ablation_after_pos, sae = sae, feature_ind=feat_to_ablate, positions=[prefix_size for _ in range(len(tokens))])\n",
    "            with Trace(model, activation_name, edit_output=hook_function) as _:\n",
    "                ablated_reward = model(tokens.to(device)).logits.cpu().squeeze()\n",
    "            reward_diff = original_reward - ablated_reward\n",
    "            all_reward_diffs.append(reward_diff)\n",
    "    all_reward_diffs = torch.stack(all_reward_diffs)\n",
    "    print('Original Memory Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    return all_reward_diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Memory Allocated: 24.0 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.8962,  0.8521,  0.6026,  0.4036,  0.1692,  0.1499,  0.0982,  0.0645,\n",
       "          0.0245, -0.0058]),\n",
       " tensor([17168, 32744, 20930, 28839, 24910,   131, 13635, 17479, 23950,  9554]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_text = \"\\n\\nHuman: What's the capital of France?\\n\\nAssistant:\"\n",
    "completions = [\n",
    "    \" Thank you. No problem. thank you thanks to you thank you\",\n",
    "]\n",
    "baselines = [\n",
    "    \" Paris is the capital of France\",\n",
    "    \" Yo mama so old, when someone told her to act her age, she died.\",\n",
    "    \" I hate you, you worthless piece of trash.\",\n",
    "]\n",
    "num_baselines = len(baselines)\n",
    "\n",
    "custom_text = [prefix_text + text for text in completions] + [prefix_text + text for text in baselines]\n",
    "tokens = tokenizer(custom_text, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "prefix_tokens = tokenizer(prefix_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "prefix_size = len(prefix_tokens[0])\n",
    "\n",
    "# Use Attribution Patching (AtP) to find approx effects. (Might take a min to run)\n",
    "approx_effects = attribution_patching(model, model_nnsight, sae, tokens, activation_name, prefix_size, tracer_kwargs=tracer_kwargs, steps=10)\n",
    "\n",
    "# Ignore features that affected reward in baselines\n",
    "ignored_baseline_effects = ignore_baseline_features(approx_effects, num_baselines, top_features_to_ignore=10)\n",
    "ignored_baseline_effects[0,:,].sum(0).topk(10), ignored_baseline_effects[0,:,].sum(0).topk(10, largest=False)\n",
    "top_pos_features = ignored_baseline_effects[0,:,].sum(0).topk(10, largest=False).indices\n",
    "top_neg_features = ignored_baseline_effects[0,:,].sum(0).topk(10).indices\n",
    "\n",
    "# Find the actual ablation effect. \n",
    "# Choose \"pos\" for positive features (e.g. Thank you. No Problem!) \n",
    "# Choose \"neg\" for negative features (e.g. repeating text)\n",
    "true_abl_effect = true_ablation_effect(top_pos_features, tokens, prefix_size)\n",
    "effects, local_ind = true_abl_effect[:, 0].sort(descending=True)\n",
    "effects, top_pos_features[local_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: 17168\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<body style=\"background-color: black; color: white;\">\n",
       "Token Activations: <span style=\"background-color:rgba(255,255,255,1);color:rgb(0,0,0)\">&nbsp0.0&nbsp</span><span style=\"background-color:rgba(187,187,255,1);color:rgb(0,0,0)\">&nbsp8.0&nbsp</span><span style=\"background-color:rgba(125,125,255,1);color:rgb(0,0,0)\">&nbsp16.0&nbsp</span><span style=\"background-color:rgba(62,62,255,1);color:rgb(255,255,255)\">&nbsp24.0&nbsp</span><span style=\"background-color:rgba(0,0,255,1);color:rgb(255,255,255)\">&nbsp32.0&nbsp</span><div style=\"margin-top: 0.5em;\"></div><span>Prompt<br></span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">\\n\\nHuman:&nbspWhat's&nbspthe&nbspcapital&nbspof&nbspFrance?\\n\\nAssistant:</span><div style=\"margin-top: 0.2em;\"></div><span>Reward: -3.04 -> -4.62 <br> 1.</span><span style=\"background-color:rgba(0,0,255,1);margin-right: 0.00em; color:rgb(255,255,255)\">&nbspThank</span><span style=\"background-color:rgba(0,0,255,1);margin-right: 0.00em; color:rgb(255,255,255)\">&nbspyou</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">.</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspNo</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspproblem</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">.</span><span style=\"background-color:rgba(25,25,255,1);margin-right: 0.00em; color:rgb(255,255,255)\">&nbspthank</span><span style=\"background-color:rgba(38,38,255,1);margin-right: 0.00em; color:rgb(255,255,255)\">&nbspyou</span><span style=\"background-color:rgba(55,55,255,1);margin-right: 0.00em; color:rgb(255,255,255)\">&nbspthanks</span><span style=\"background-color:rgba(123,123,255,1);margin-right: 0.00em; color:rgb(255,255,255)\">&nbspto</span><span style=\"background-color:rgba(181,181,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspyou</span><span style=\"background-color:rgba(165,165,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspthank</span><span style=\"background-color:rgba(223,223,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspyou</span><div style=\"margin-top: 0.2em;\"></div><span>Reward: -2.56 -> -2.56 <br> 2.</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspParis</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspis</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspthe</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspcapital</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspof</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspFrance</span><div style=\"margin-top: 0.2em;\"></div><span>Reward: -8.31 -> -8.31 <br> 3.</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspYo</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspm</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">ama</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspso</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspold</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">,</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspwhen</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspsomeone</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbsptold</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspher</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspto</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspact</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspher</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspage</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">,</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspshe</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspdied</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">.</span><div style=\"margin-top: 0.2em;\"></div><span>Reward: -7.32 -> -7.32 <br> 4.</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspI</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbsphate</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspyou</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">,</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspyou</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspworthless</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbsppiece</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbspof</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">&nbsptrash</span><span style=\"background-color:rgba(255,255,255,1);margin-right: 0.00em; color:rgb(0,0,0)\">.</span><div style=\"margin-top: 0.2em;\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_feature = top_pos_features[local_ind][0]\n",
    "ablate_these_features = [target_feature, top_pos_features[local_ind][1]]\n",
    "display_feature_activation_and_ablation(tokens, prefix_text, target_feature, ablate_these_features, model, sae, activation_name, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/sae-rm/logan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/root/sae-rm/logan/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:19<00:00,  6.63s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"reciprocate/dahoas-gptj-rm-static\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"/root/dictionary_learning/\")\n",
    "from dictionary import GatedAutoEncoder\n",
    "\n",
    "layer = 2\n",
    "activation_name = f\"transformer.h.{layer}\"\n",
    "sae_file = f\"saes/ae_layer{layer}.pt\"\n",
    "ae = GatedAutoEncoder.from_pretrained(sae_file).to(device)\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch \n",
    "hh = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\")\n",
    "token_length_cutoff = 870 # 99% of chosen data\n",
    "\n",
    "# Remove datapoints longer than a specific token_length\n",
    "# Check if file exists\n",
    "index_file_name = \"rm_save_files/index_small_enough.pt\"\n",
    "dataset_size = hh.num_rows\n",
    "if os.path.exists(index_file_name):\n",
    "    index_small_enough = torch.load(index_file_name)\n",
    "else:\n",
    "    print(\"hey\")\n",
    "#     index_small_enough = torch.ones(dataset_size, dtype=torch.bool)\n",
    "# # \n",
    "#     for ind, text in enumerate(tqdm(hh)):\n",
    "#         chosen_text = text[\"chosen\"]\n",
    "#         rejected_text = text[\"rejected\"]\n",
    "#         #convert to tokens\n",
    "#         length_chosen = len(tokenizer(chosen_text)[\"input_ids\"])\n",
    "#         length_rejected = len(tokenizer(rejected_text)[\"input_ids\"])\n",
    "#         if length_chosen > token_length_cutoff or length_rejected > token_length_cutoff:\n",
    "#             index_small_enough[ind] = False\n",
    "#     # Save the indices\n",
    "#     torch.save(index_small_enough, \"rm_save_files/index_small_enough.pt\")\n",
    "\n",
    "hh = hh.select(index_small_enough.nonzero()[:, 0])\n",
    "top_reward_diff_ind = torch.load(\"rm_save_files/top_reward_diff_ind.pt\")\n",
    "hh = hh.select(top_reward_diff_ind)\n",
    "\n",
    "# select first 100 datapoints\n",
    "hh = hh.select(range(1000))\n",
    "batch_size = 12\n",
    "hh_dl = DataLoader(hh, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 243.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subsets: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_datapoints = len(hh)\n",
    "token_length_cutoff = 871\n",
    "\n",
    "index_of_chosen_rejection_difference = torch.zeros(num_datapoints, dtype=torch.int16)\n",
    "\n",
    "# Assuming hh_dl is a DataLoader that returns batches of data\n",
    "subsets = 0\n",
    "for i, batch in enumerate(tqdm(hh)):\n",
    "    chosen_texts = batch[\"chosen\"]\n",
    "    rejected_texts = batch[\"rejected\"]\n",
    "\n",
    "    # Tokenize texts in batches\n",
    "    chosen_tokens = tokenizer(chosen_texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=token_length_cutoff)[\"input_ids\"]\n",
    "    rejected_tokens = tokenizer(rejected_texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=token_length_cutoff)[\"input_ids\"]\n",
    "\n",
    "    chosen_token_original_length = tokenizer(chosen_texts, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "    rejected_token_original_length = tokenizer(rejected_texts, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "    min_length = min(chosen_token_original_length, rejected_token_original_length)\n",
    "\n",
    "    # Compare tokens and find divergence points\n",
    "    divergence_matrix = (chosen_tokens != rejected_tokens).to(torch.int)  # Matrix of 1s where tokens differ\n",
    "\n",
    "    # Find the first divergence index for each pair of texts\n",
    "    divergence_indices = divergence_matrix.argmax(dim=1)\n",
    "    if divergence_indices == min_length:\n",
    "        subsets += 1\n",
    "        divergence_indices -= 1\n",
    "\n",
    "    # Calculate start index for the current batch\n",
    "    # start_index = i * batch_size\n",
    "    # end_index = start_index + len(chosen_texts)\n",
    "\n",
    "    # Store the divergence indices in the appropriate positions\n",
    "    # index_of_chosen_rejection_difference[start_index:end_index] = divergence_indices\n",
    "    index_of_chosen_rejection_difference[i] = divergence_indices\n",
    "print(f\"Number of subsets: {subsets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(index_of_chosen_rejection_difference, \"rm_save_files/index_of_chosen_rejection_difference.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/84 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n",
      "100%|██████████| 84/84 [12:37<00:00,  9.02s/it]\n"
     ]
    }
   ],
   "source": [
    "from baukit import Trace\n",
    "from einops import rearrange\n",
    "num_features = ae.encoder.weight.shape[0]\n",
    "num_datapoints = len(hh)\n",
    "max_feature_activations_chosen = torch.zeros(num_datapoints, num_features)\n",
    "max_feature_activations_rejected = torch.zeros(num_datapoints, num_features)\n",
    "\n",
    "chosen_rejected_list = [max_feature_activations_chosen, max_feature_activations_rejected]\n",
    "with torch.no_grad():\n",
    "    for batch_ind, batch in tqdm(enumerate(hh_dl), total=len(hh_dl)):\n",
    "        chosen_texts = batch[\"chosen\"]\n",
    "        rejected_texts = batch[\"rejected\"]\n",
    "        chosen_tokens = tokenizer(chosen_texts, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "        rejected_tokens = tokenizer(rejected_texts, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "        # Get Intermediate Activations\n",
    "        index_of_token_diff = index_of_chosen_rejection_difference[batch_ind*batch_size:(batch_ind+1)*batch_size].to(torch.int)\n",
    "        for chos_rej_ind, batch_tokens in enumerate([chosen_tokens, rejected_tokens]):\n",
    "\n",
    "            # Get intermediate activations\n",
    "            with Trace(model, activation_name) as ret:\n",
    "                _ = model(batch_tokens.to(device)).logits\n",
    "                internal_activations = ret.output\n",
    "                # check if instance tuple\n",
    "                if(isinstance(internal_activations, tuple)):\n",
    "                    internal_activations = internal_activations[0]\n",
    "\n",
    "            # Get Features for activation\n",
    "            current_batch_size = batch_tokens.shape[0]\n",
    "            batched_neuron_activations = rearrange(internal_activations, \"b s n -> (b s) n\" )\n",
    "            batched_dictionary_activations = ae.encode(batched_neuron_activations)\n",
    "            batched_feature_activations = rearrange(batched_dictionary_activations, \"(b s) n -> b s n\", b=current_batch_size)\n",
    "            # Store only max over the differing tokens\n",
    "            # ensure shape of index_of_token_diff is same as batched_feature_activations\n",
    "            # try:\n",
    "            max_feature_act = torch.max(batched_feature_activations[:, index_of_token_diff, :], dim=1).values.cpu()\n",
    "            chosen_rejected_list[chos_rej_ind][batch_ind*batch_size:(batch_ind+1)*batch_size] = max_feature_act\n",
    "                # del max_feature_act\n",
    "            # except:\n",
    "            #     print(f\"Text at batch {batch_ind} and index {chos_rej_ind} are subset of each other. Skipping...\")\n",
    "                # print(f\"Chosen text: {chosen_texts}\")\n",
    "                # print(f\"Rejected text: {rejected_texts}\")\n",
    "            \n",
    "            del ret, internal_activations, batched_neuron_activations, batched_dictionary_activations, batched_feature_activations, max_feature_act\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than 0: 4335\n",
      "More than 10: 2368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4335/4335 [00:00<00:00, 19003.39it/s]\n"
     ]
    }
   ],
   "source": [
    "#combine both chosen and rejected. Find the top-k datapoints for each feature. Ignore Features that don't have j activations (& print so)\n",
    "chosen_rejected = torch.cat(chosen_rejected_list, dim=0)\n",
    "k = 10\n",
    "j = 10\n",
    "nz_feat = (chosen_rejected > 0).sum(0)\n",
    "print(f\"More than 0: {nz_feat.count_nonzero()}\")\n",
    "print(f\"More than {j}: {(nz_feat > j).count_nonzero()}\")\n",
    "\n",
    "nz_feature_ind = nz_feat.nonzero().squeeze()\n",
    "num_nz_features = nz_feature_ind.shape[0]\n",
    "feature_top_k = torch.zeros(num_nz_features, k)\n",
    "\n",
    "for feature_ind, nz_feat_i in tqdm(enumerate(nz_feature_ind), total=num_nz_features):\n",
    "    feature_activations = chosen_rejected[:, nz_feat_i]\n",
    "    top_k_activations = torch.topk(feature_activations, k).indices\n",
    "    feature_top_k[feature_ind] = top_k_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "# # save nz_feature_ind\n",
    "torch.save(nz_feature_ind, \"rm_save_files/nz_feature_ind.pt\")\n",
    "torch.save(feature_top_k, \"rm_save_files/each_nz_features_top_activating_datapoints.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4335, 10]),\n",
       " tensor([    6,    11,    15,  ..., 32739, 32743, 32748]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_top_k.shape, nz_feature_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz_ind = nz_feat.nonzero()[:, 0]\n",
    "chosen_rejected_nz = chosen_rejected[:, nz_ind]\n",
    "chosen_rejected_nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.sum(chosen_rejected, dim=0)>0).count_nonzero(), (torch.sum(chosen_rejected, dim=0) > 10).count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_rejected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(feature_activations, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(batched_feature_activations[:, index_of_token_diff, :], dim=1).values.count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id, tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.sep_token_id, tokenizer.cls_token_id, tokenizer.mask_token_id, tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_token_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_dictionary_activations.shape, internal_activations.shape, hh_dl.batch_size, len(chosen_texts), len(rejected_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, b in enumerate(hh_dl):\n",
    "    print(len(b[\"chosen\"]))\n",
    "    if(ind > 100):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
